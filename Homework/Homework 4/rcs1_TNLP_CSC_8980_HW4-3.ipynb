{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "rcs1_TNLP_CSC_8980_HW4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNl8_ZaJtv_A"
      },
      "source": [
        "## Rajath Chikkatur Srinivasa\n",
        "## 002552425"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tzMwn1WuhxV",
        "outputId": "9537751e-77f1-4820-ab2d-bb13851ae75a"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install vaderSentiment\n",
        "# imports\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "import sklearn.metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "s_words = stopwords.words('english')\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmWWOuUgm0-U"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "clear_output()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MixcMtUSyBoQ"
      },
      "source": [
        "#### Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udo_PMQiugCo"
      },
      "source": [
        "# unzipping dataset, you can use same GDrive link\n",
        "!unzip /content/drive/MyDrive/Projects/NLP/trainingandtestdata.zip\n",
        "clear_output()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzUt7Sb_VoHt",
        "outputId": "f40f89b2-ca99-4e03-cfbc-65e11b58d9a4"
      },
      "source": [
        "# mounting Gdrive, please authenticate with the ID\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV0W6JymVRw-"
      },
      "source": [
        "# reading datasets\n",
        "testdata = pd.read_csv('testdata.manual.2009.06.14.csv', names=['polarity', 'tweet_id', 'tweet_date', 'query', 'user_name', 'tweet_text'])\n",
        "traindata = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', names=['polarity', 'tweet_id', 'tweet_date', 'query', 'user_name', 'tweet_text'],low_memory=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysBrFEB2dctP"
      },
      "source": [
        "### 1. Take the positive and the negative tweets only. Use Sklearn to split the dataset in 80% training, 20% testing splits. Provide a nicely formatted summary of these splits, containing their size) (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4CCE_Tzt0Iz"
      },
      "source": [
        "# taking positive and negative tweets only\n",
        "traindata.query('polarity == \"0\" or polarity == \"4\"', inplace = True)\n",
        "# splitting the dataset\n",
        "# np.random.seed(2361)\n",
        "# converting 4 to 1 to reduce memory usage\n",
        "traindata['polarity'] = traindata['polarity'].astype(str)\n",
        "traindata['polarity'] = traindata['polarity'].replace(['4'], '1')\n",
        "traindata['polarity'] = traindata['polarity'].astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(traindata.tweet_text.tolist(), traindata.polarity.tolist(),train_size=0.80,random_state=2361)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYhTpxcfDT3y"
      },
      "source": [
        "# y_train = y_train.to_frame()\n",
        "# y_test = y_test.to_frame()\n",
        "# split_size = [X_train.size, X_test.size, y_train.size, y_test.size]\n",
        "# # split_shape = [X_train.shape, X_test.shape, y_train.shape, y_test.shape]\n",
        "# # split_info = [X_train.info(), X_test.info(), y_train.info(), y_test.info()]\n",
        "split_size = [len(X_train), len(X_test), len(y_train), len(y_test)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "QxcnHBH7wFKO",
        "outputId": "a38f4afd-5529-4d89-9785-234517d52273"
      },
      "source": [
        "df = pd.DataFrame({'split_size':split_size}, index = ['X_train.size', 'X_test.size', 'y_train.size', 'y_test.size'])\n",
        "df"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_train.size</th>\n",
              "      <td>1280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_test.size</th>\n",
              "      <td>320000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y_train.size</th>\n",
              "      <td>1280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y_test.size</th>\n",
              "      <td>320000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              split_size\n",
              "X_train.size     1280000\n",
              "X_test.size       320000\n",
              "y_train.size     1280000\n",
              "y_test.size       320000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlX__d0st7wA"
      },
      "source": [
        "###  Q2. Use the code from the previous classes to build the following models (15 points):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lynSgY3yxL_N"
      },
      "source": [
        "#### SVM Using TF-IDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp53ZmlwyXA4"
      },
      "source": [
        "# y_train_values = y_train.values.ravel()\n",
        "# y_train_values = y_train_values.astype(np.int8)\n",
        "# print(y_train_values.nbytes, y_train_values.dtype)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtKc4EoiVRxA",
        "outputId": "74b31aa1-c44f-42ff-a120-552de1260baf"
      },
      "source": [
        "np.random.seed(2361)\n",
        "model_svm = make_pipeline(TfidfVectorizer(), SVC(kernel='linear', gamma='auto', max_iter=15000))\n",
        "model_svm.fit(X_train, y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='auto', kernel='linear', max_iter=15000,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5mtOXk5xUbI"
      },
      "source": [
        "#### Naive Bayes using TF-IDF.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CUyWKn8xjDJ",
        "outputId": "c63e0824-0223-47a4-e93b-6dded25e6e3a"
      },
      "source": [
        "np.random.seed(2361)\n",
        "model_nb = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_nb.fit(X_train, y_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75_ugUifxXzj"
      },
      "source": [
        "#### Random Forest using TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3Jlb5HtzqiR",
        "outputId": "f56391fe-6ed3-4365-9885-7aba75f455dc"
      },
      "source": [
        "# np.random.seed(2361)\n",
        "model_rf = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_jobs=-1, warm_start=True, max_depth=75))\n",
        "model_rf.fit(X_train, y_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=75, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=-1,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrVBaXWiuAf2"
      },
      "source": [
        "### 3. Use the code from the LSTM class to build a classifier for negative and positive sentiment tweets. Train the model with the training data split. Once the model is built, test it with the testing data split. Display the classifier report for this evaluation. Answer the following question: What can you say about the performance of this model? (40 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs0B1hdyVRxC"
      },
      "source": [
        "# Map for readable classnames\n",
        "class_names = [0,1]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYRXXxs_VRxC"
      },
      "source": [
        "# combining to find unique terms \n",
        "# data = traindata.tweet_text\n",
        "data = X_train + X_test"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "6t40hhoYVRxC",
        "outputId": "2d14ef09-82ec-4d6f-d9de-5f4337817d5a"
      },
      "source": [
        "# PREPROCESSING\n",
        "# lstm does not understand the words we need to encode it.\n",
        "word_index = {}\n",
        "# vocab_size = len(word_index)\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNKNOWN>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "# encoding via tokenizing is providing very low accuracy.\n",
        "\"\"\"combine_data = ' '.join(data)\n",
        "tokenized_data = word_tokenize(combine_data)\n",
        "unique_tokens = set(tokenized_data)\"\"\"\n",
        "# encoding manually by looping through each sentence.\n",
        "\"\"\"start_count = 4 # because 4 keyvalues are set from 0-3\n",
        "data = ''.join(data)\n",
        "data = data.split(' ')\n",
        "data = list(set(data))\n",
        "for i in data:\n",
        "    if i not in word_index:\n",
        "        word_index[i] = start_count\n",
        "        start_count+=1\"\"\"\n",
        "\n",
        "# nested looping through tweets and words\n",
        "start_count = 4\n",
        "for i in data:\n",
        "  for j in i.split(' '):\n",
        "    if j not in word_index:\n",
        "      word_index[j] = start_count\n",
        "      start_count += 1\n",
        "\n",
        "\n",
        "# encoding technique-1# perform encoding by allocating random integers to each unique word\n",
        "\"\"\"from random import randint\n",
        "i = 4\n",
        "testing = [i += 1 word_index[word] = i for i in data for j in i.split(' ') if j not in word_index]\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"from random import randint\\ni = 4\\ntesting = [i += 1 word_index[word] = i for i in data for j in i.split(' ') if j not in word_index]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bpy4YxFVRxD"
      },
      "source": [
        "# splitting the encoded values \n",
        "\n",
        "\"\"\"\n",
        "X_train_encode, X_test_encode = train_test_split(list(word_index.values),random_state=2361)\n",
        "\"\"\"\n",
        "X_train_encode=[]\n",
        "for tweet in X_train:\n",
        "    xtrain_list = []\n",
        "    for i in tweet.split(' '):\n",
        "      xtrain_list.append(word_index[i])\n",
        "        # try:\n",
        "  \n",
        "        # except:\n",
        "            # continue\n",
        "    X_train_encode.append(xtrain_list)\n",
        "X_test_encode=[]\n",
        "for ttweet in X_test:\n",
        "    xtest_list = []\n",
        "    for i in ttweet.split(' '):\n",
        "      xtest_list.append(word_index[i])\n",
        "        # try:\n",
        "            \n",
        "        # except:\n",
        "        #     continue\n",
        "    X_test_encode.append(xtest_list)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCWElHG4VRxD",
        "outputId": "29290c22-1f99-4fa6-e276-f7c9da199875"
      },
      "source": [
        "#converting back the train_test split list variable to nDarray\n",
        "# X_train, X_test, y_train, y_test = np.ravel(X_train), np.ravel(X_test), np.ravel(y_train), np.ravel(y_test)\n",
        "X_train = np.array(X_train_encode)\n",
        "X_test = np.array(X_test_encode) \n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# The length of tweets are of maximum 280 characters\n",
        "review_length = 280\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen = review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen = review_length)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uirugrVWVRxE",
        "outputId": "f98b568d-c7fc-4f02-ef5f-f49af8a31400"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = len(word_index), # The size of our vocabulary \n",
        "        output_dim = 32, # Dimensions to which each words shall be mapped\n",
        "        input_length = review_length # Length of input sequences\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.LSTM(\n",
        "        units=32 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=1, # Single unit\n",
        "        activation='sigmoid' # Sigmoid activation function (output from 0 to 1)\n",
        "    )\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 32)           43217536  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 300, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 43,225,889\n",
            "Trainable params: 43,225,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JASmQfIfVRxE",
        "outputId": "d564aba3-ed72-4744-d951-8591aa11f80f"
      },
      "source": [
        "y_tensor = tf.convert_to_tensor(X_train, dtype=tf.int32)\n",
        "y_tensor1 = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "history = model.fit(y_tensor, y_tensor1,batch_size=256,epochs=3,validation_split=0.2,verbose=1)\n",
        "# history = model.fit(X_train, y_train,batch_size=256,epochs=3,validation_split=0.2,verbose=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "4000/4000 [==============================] - 1578s 387ms/step - loss: 0.4777 - accuracy: 0.7666 - val_loss: 0.3830 - val_accuracy: 0.8279\n",
            "Epoch 2/3\n",
            "4000/4000 [==============================] - 1561s 390ms/step - loss: 0.2921 - accuracy: 0.8778 - val_loss: 0.4038 - val_accuracy: 0.8209\n",
            "Epoch 3/3\n",
            "4000/4000 [==============================] - 1567s 392ms/step - loss: 0.1882 - accuracy: 0.9267 - val_loss: 0.4545 - val_accuracy: 0.8123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKH11JdGVRxE",
        "outputId": "78a7beda-796f-498d-89f7-d06efdd5c7ff"
      },
      "source": [
        "predicted_classes = model.predict_classes(X_test)\n",
        "classification_report = classification_report(y_test, predicted_classes) # 0 - negative, 1 - positive\n",
        "print(classification_report)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.83      0.82    159998\n",
            "           1       0.82      0.80      0.81    160002\n",
            "\n",
            "    accuracy                           0.81    320000\n",
            "   macro avg       0.81      0.81      0.81    320000\n",
            "weighted avg       0.81      0.81      0.81    320000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DkmKYt0mYQ1"
      },
      "source": [
        "lstm_precision=0.81\n",
        "lstm_recall=0.81\n",
        "lstm_f1 = 0.81\n",
        "lstm_metrics = [lstm_precision, lstm_recall, lstm_f1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "5mLUSaqSmlff",
        "outputId": "1d87a24f-d503-4767-c097-5ccc2a82142b"
      },
      "source": [
        "df1 = pd.DataFrame(lstm_metrics, index=['lstm_precision', 'lstm_recall', 'lstm_f1'])\n",
        "df1"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lstm_precision</th>\n",
              "      <td>0.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lstm_recall</th>\n",
              "      <td>0.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lstm_f1</th>\n",
              "      <td>0.81</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0\n",
              "lstm_precision  0.81\n",
              "lstm_recall     0.81\n",
              "lstm_f1         0.81"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvf53bRsVRxE"
      },
      "source": [
        "#### What can you say about the performance of this model?\n",
        " - The model performs better than the traditional models\n",
        " - We can furhter plot ROC AUC curve to measure the performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6nqDGwWuFd3"
      },
      "source": [
        "### 4. Compare all models together in terms of Precision, Recall and F1 score. Put all of these numbers in a nicely formatted dataframe. Answer the following questions: Which model performs the best? Why do you think this is? What do you think you can do to improve performance? (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKL9l4Iqgt7e",
        "outputId": "32ee7075-a72e-406e-a748-77cbee75ee3f"
      },
      "source": [
        "# SVM Metrics\n",
        "labels = model_svm.predict(X_test)\n",
        "svm_precision = sklearn.metrics.precision_score(labels,y_test)\n",
        "svm_recall = sklearn.metrics.recall_score(labels,y_test)\n",
        "svm_f1 = sklearn.metrics.f1_score(labels,y_test)\n",
        "svm_metrics = [svm_precision,svm_recall, svm_f1]\n",
        "svm_metrics"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7255909558067831, 0.7338877338877339, 0.7297157622739018]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbKiL82hVRxE",
        "outputId": "f57f58b2-9ccd-4d40-a980-00a5f6b2892a"
      },
      "source": [
        "# Naive Bayes Metrics\n",
        "labels = model_nb.predict(X_test)\n",
        "nb_precision = sklearn.metrics.precision_score(labels,y_test)\n",
        "nb_recall = sklearn.metrics.recall_score(labels,y_test)\n",
        "nb_f1 = sklearn.metrics.f1_score(labels,y_test)\n",
        "nb_metrics = [nb_precision,nb_recall, nb_f1]\n",
        "nb_metrics"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6783144912641316, 0.7560137457044673, 0.7150595882990249]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMgpVl1NuKwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec2672e-d49b-4d0a-8f73-86f208a88b81"
      },
      "source": [
        "# Random Forest Metrics\n",
        "labels = model_rf.predict(X_test)\n",
        "rf_precision = sklearn.metrics.precision_score(labels,y_test)\n",
        "rf_recall = sklearn.metrics.recall_score(labels,y_test)\n",
        "rf_f1 = sklearn.metrics.f1_score(labels,y_test)\n",
        "rf_metrics = [rf_precision,rf_recall, rf_f1]\n",
        "rf_metrics"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7194244604316546, 0.7128309572301426, 0.7161125319693095]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk6U8cmcldF3",
        "outputId": "b74dd343-6ef0-4758-d7a7-451aa51fec44"
      },
      "source": [
        "# # lstm metrics\n",
        "lstm_metrics = [lstm_precision, lstm_recall, lstm_f1]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.81, 0.81, 0.81]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "UQAXERwzVRxF",
        "outputId": "1ddf4e9c-99da-45c3-f25e-baee87b8c535"
      },
      "source": [
        "metrics_df = pd.DataFrame({'lstm_metrics':lstm_metrics,'svm':svm_metrics, 'naive_bayes':nb_metrics, 'random_forest':rf_metrics},  index = ['precision', 'recall', 'F1'])\n",
        "metrics_df"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lstm_metrics</th>\n",
              "      <th>svm</th>\n",
              "      <th>naive_bayes</th>\n",
              "      <th>random_forest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.725591</td>\n",
              "      <td>0.678314</td>\n",
              "      <td>0.719424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.733888</td>\n",
              "      <td>0.756014</td>\n",
              "      <td>0.712831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F1</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.729716</td>\n",
              "      <td>0.715060</td>\n",
              "      <td>0.716113</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           lstm_metrics       svm  naive_bayes  random_forest\n",
              "precision          0.81  0.725591     0.678314       0.719424\n",
              "recall             0.81  0.733888     0.756014       0.712831\n",
              "F1                 0.81  0.729716     0.715060       0.716113"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eJ-9JCoVRxF"
      },
      "source": [
        "#### Which Model Performs the best\n",
        "- LSTM Performs the best, among SVM,Random Forest and NB: SVM performs best.\n",
        "\n",
        "#### Why do you think this is the best model?\n",
        "- Based on  recall, precision and f1 LSTM performs best because NB considers all features as independent variables. \n",
        "- RandomForest overfits with propotional increase in data size.\n",
        "- SVM is out to be the best but since LSTM is neural network based model it is the best model\n",
        "\n",
        "#### What do you think you can do to improve performance?\n",
        "- We can tune hyper parameters and normalize the input values.\n",
        "- We can tune the number of epocs and number of nerouns passed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYao0RIauK6p"
      },
      "source": [
        "### 5. Add to the comparison of \\#4 a the manually calculated precision, recall and F1 score using VADER and their suggested defaults to categorize the test split tweets in positive or negative. Answer the following questions: Is this approach as good as the previous ones? Why do you think this is? (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZRpjotOsa38"
      },
      "source": [
        "vader_polarity = []\n",
        "for tweet in X_test:\n",
        "  sent = analyzer.polarity_scores(tweet)\n",
        "  if sent['compound'] >= 0.05:\n",
        "    vader_polarity.append(1)\n",
        "  else:\n",
        "    vader_polarity.append(0)\n",
        "\"\"\"[vader_polarity.append(1) if (sent['compound'] >= 0.05) else vader_polarity.append(0) for tweet in X_test sent = analyzer.polarity_scores(tweet)]\n",
        "  vader_polarity.append(0) \n",
        "  if sent['compound'] >= 0.05  and sent['compound'] < 0.05:\n",
        "    vader_polarity.append(-1) # - 1 for neutral\"\"\""
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2aeMYqZ192g"
      },
      "source": [
        "# TN, TP, FP,FN = confusion_matrix(y_test, vader_polarity).ravel()\n",
        "# print(TN, TP, FP,FN)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBE5tMt7uQZC"
      },
      "source": [
        "\n",
        "recall = sklearn.metrics.recall_score(vader_polarity,y_test)\n",
        "pre =  sklearn.metrics.precision_score(vader_polarity,y_test)\n",
        "f1 =  sklearn.metrics.f1_score(vader_polarity,y_test)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqWJKvPQtb6y",
        "outputId": "300213d8-3cf5-4798-c974-874f28fa05f3"
      },
      "source": [
        "\n",
        "vader_metrics = [recall, pre, f1]\n",
        "vader_metrics"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6644775841053957, 0.6235172060349246, 0.643346090971526]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Z8dKcW_s9lRh",
        "outputId": "409041d1-b85a-440b-d148-3444282779f8"
      },
      "source": [
        "metrics_df['vader_metrics'] = vader_metrics\n",
        "metrics_df"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lstm_metrics</th>\n",
              "      <th>svm</th>\n",
              "      <th>naive_bayes</th>\n",
              "      <th>random_forest</th>\n",
              "      <th>vader_metrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.725591</td>\n",
              "      <td>0.678314</td>\n",
              "      <td>0.719424</td>\n",
              "      <td>0.664478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.733888</td>\n",
              "      <td>0.756014</td>\n",
              "      <td>0.712831</td>\n",
              "      <td>0.623517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.729716</td>\n",
              "      <td>0.715060</td>\n",
              "      <td>0.716113</td>\n",
              "      <td>0.643346</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   lstm_metrics       svm  naive_bayes  random_forest  vader_metrics\n",
              "0          0.81  0.725591     0.678314       0.719424       0.664478\n",
              "1          0.81  0.733888     0.756014       0.712831       0.623517\n",
              "2          0.81  0.729716     0.715060       0.716113       0.643346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqgvFJk2VRxF"
      },
      "source": [
        "#### Is this approach as good as the previous one?\n",
        "No, the vader measurement doesnt provide performance.\n",
        "\n",
        "#### Why do you think this is?\n",
        "Vader sentiment is a rule based technique and the polarity is pre-defines. hence testing the test split on this is not the right way to go about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l09j81TuQhM"
      },
      "source": [
        "### Bonus (30 points): Try the following things to improve the LSTM model:\n",
        "1) Use 90% training data, 10% testing \\\n",
        "2) Remove stopwords from the tweets.\\\n",
        "3) Remove all user mentions for the tweets (@something) \\\n",
        "Compare all three new models in terms of their precision, recall and F1 score. \\\n",
        "\n",
        "Answer the following questions: Did this change the results in any way? Why do you think so?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM2sJP6M0koT"
      },
      "source": [
        "X_train, X_test, y_train1, y_test = train_test_split(traindata.tweet_text.tolist(), traindata.polarity.tolist(),test_size=0.10,random_state=2361)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh-JvIzULSVO"
      },
      "source": [
        "X_train_encode=[]\n",
        "for tweet in X_train:\n",
        "    xtrain_list = []\n",
        "    for i in tweet.split(' '):\n",
        "      xtrain_list.append(word_index[i])\n",
        "        # try:\n",
        "  \n",
        "        # except:\n",
        "            # continue\n",
        "    X_train_encode.append(xtrain_list)\n",
        "X_test_encode=[]\n",
        "for ttweet in X_test:\n",
        "    xtest_list = []\n",
        "    for i in ttweet.split(' '):\n",
        "      xtest_list.append(word_index[i])\n",
        "        # try:\n",
        "            \n",
        "        # except:\n",
        "        #     continue\n",
        "    X_test_encode.append(xtest_list)\n",
        "\n",
        "#converting back the train_test split list variable to nDarray\n",
        "# X_train, X_test, y_train, y_test = np.ravel(X_train), np.ravel(X_test), np.ravel(y_train), np.ravel(y_test)\n",
        "X_train = np.array(X_train_encode, dtype=object)\n",
        "X_test = np.array(X_test_encode, dtype=object) \n",
        "y_train1 = np.array(y_train1, dtype=object)\n",
        "y_test = np.array(y_test, dtype=object)\n",
        "\n",
        "# The length of tweets are of maximum 280 characters\n",
        "review_length = 280\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen = review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen = review_length)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLgRtOqvVRxG",
        "outputId": "f75bdef9-8b1a-4c08-9fd4-fb41328b6c9f"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = len(word_index), # The size of our vocabulary \n",
        "        output_dim = 32, # Dimensions to which each words shall be mapped\n",
        "        input_length = review_length # Length of input sequences\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.LSTM(\n",
        "        units=32 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=1, # Single unit\n",
        "        activation='sigmoid' # Sigmoid activation function (output from 0 to 1)\n",
        "    )\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "model.summary()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 280, 32)           43217536  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 280, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 43,225,889\n",
            "Trainable params: 43,225,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdJw4lgbVRxG",
        "outputId": "0016a962-1255-4054-a27d-235c3b033025"
      },
      "source": [
        "y_tensor = tf.convert_to_tensor(X_train, dtype=tf.int32)\n",
        "y_tensor1 = tf.convert_to_tensor(y_train1, dtype=tf.int32)\n",
        "history = model.fit(y_tensor, y_tensor1,batch_size=256,epochs=3,validation_split=0.2,verbose=1)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "4500/4500 [==============================] - 1811s 395ms/step - loss: 0.4752 - accuracy: 0.7677 - val_loss: 0.3818 - val_accuracy: 0.8291\n",
            "Epoch 2/3\n",
            "4500/4500 [==============================] - 1780s 396ms/step - loss: 0.2929 - accuracy: 0.8773 - val_loss: 0.3951 - val_accuracy: 0.8268\n",
            "Epoch 3/3\n",
            "4500/4500 [==============================] - 1782s 396ms/step - loss: 0.1923 - accuracy: 0.9247 - val_loss: 0.4452 - val_accuracy: 0.8136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBy3VqKTVRxG"
      },
      "source": [
        "y_test = y_test.astype(int)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvz-BeC7q-XX",
        "outputId": "90a7c84c-42a3-4114-9818-bf011dac2602"
      },
      "source": [
        "predicted_classes = model.predict_classes(X_test)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEp_oPnUtUuW",
        "outputId": "4d7686d4-e19e-4c85-a763-2cd703d101b9"
      },
      "source": [
        "classification_report = classification_report(y_test, predicted_classes)\n",
        "print(classification_report)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84      0.82     80144\n",
            "           1       0.83      0.79      0.81     79856\n",
            "\n",
            "    accuracy                           0.82    160000\n",
            "   macro avg       0.82      0.82      0.82    160000\n",
            "weighted avg       0.82      0.82      0.82    160000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3acfiIg2rlhB",
        "outputId": "64f335bf-c6b1-4e1e-c369-7952b2a347bc"
      },
      "source": [
        "# considering weighted average\n",
        "lstm_precision = 0.82\n",
        "lstm_recall = 0.82\n",
        "lstm_f1 = 0.82\n",
        "\n",
        "bonus_lstm1_metrics = [lstm_precision, lstm_recall, lstm_f1]\n",
        "bonus_lstm1_metrics"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.82, 0.82, 0.82]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIrnticMVRxH"
      },
      "source": [
        "#### Removing StopWords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G3NZqK5VRxH"
      },
      "source": [
        "traindata['no_stopwords'] = traindata.tweet_text.apply(lambda x: ' '.join([word for word in x.split() if word not in (s_words)]))\n",
        "X_train = traindata['no_stopwords'].tolist()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, traindata.polarity.tolist(),test_size=0.10,random_state=2361)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaY_LJGEVRxH"
      },
      "source": [
        "X_train_encode=[]\n",
        "for tweet in X_train:\n",
        "    xtrain_list = []\n",
        "    for i in tweet.split(' '):\n",
        "      xtrain_list.append(word_index[i])\n",
        "        # try:\n",
        "  \n",
        "        # except:\n",
        "            # continue\n",
        "    X_train_encode.append(xtrain_list)\n",
        "X_test_encode=[]\n",
        "for ttweet in X_test:\n",
        "    xtest_list = []\n",
        "    for i in ttweet.split(' '):\n",
        "      xtest_list.append(word_index[i])\n",
        "        # try:\n",
        "            \n",
        "        # except:\n",
        "        #     continue\n",
        "    X_test_encode.append(xtest_list)\n",
        "\n",
        "#converting back the train_test split list variable to nDarray\n",
        "# X_train, X_test, y_train, y_test = np.ravel(X_train), np.ravel(X_test), np.ravel(y_train), np.ravel(y_test)\n",
        "X_train = np.array(X_train_encode, dtype=object)\n",
        "X_test = np.array(X_test_encode, dtype=object) \n",
        "y_train = np.array(y_train, dtype=object)\n",
        "y_test = np.array(y_test, dtype=object)\n",
        "\n",
        "# The length of tweets are of maximum 280 characters\n",
        "review_length = 280\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen = review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen = review_length)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etdmVMLpVRxH",
        "outputId": "1c1b7bb3-e01b-44b0-93d5-68e77508d9fb"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = len(word_index), # The size of our vocabulary \n",
        "        output_dim = 32, # Dimensions to which each words shall be mapped\n",
        "        input_length = review_length # Length of input sequences\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.LSTM(\n",
        "        units=32 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=1, # Single unit\n",
        "        activation='sigmoid' # Sigmoid activation function (output from 0 to 1)\n",
        "    )\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 280, 32)           43217536  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 280, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 43,225,889\n",
            "Trainable params: 43,225,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2_0_1TNVRxH",
        "outputId": "61b40026-ee37-4d23-f3a8-e011f528db8a"
      },
      "source": [
        "y_tensor = tf.convert_to_tensor(X_train, dtype=tf.int32)\n",
        "y_tensor1 = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "history = model.fit(y_tensor, y_tensor1,batch_size=256,epochs=3,validation_split=0.2,verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "4000/4000 [==============================] - 1997s 492ms/step - loss: 0.4767 - accuracy: 0.7670 - val_loss: 0.3854 - val_accuracy: 0.8271\n",
            "Epoch 2/3\n",
            "4000/4000 [==============================] - 1959s 490ms/step - loss: 0.2928 - accuracy: 0.8771 - val_loss: 0.3961 - val_accuracy: 0.8244\n",
            "Epoch 3/3\n",
            "4000/4000 [==============================] - 1954s 488ms/step - loss: 0.1892 - accuracy: 0.9264 - val_loss: 0.4557 - val_accuracy: 0.8141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5bn_DJnVRxI",
        "outputId": "3b89f592-3874-4062-d4ad-717becd1b15c"
      },
      "source": [
        "y_test = y_test.astype(int)\n",
        "predicted_classes = model.predict_classes(X_test)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUq0t34IqkVz",
        "outputId": "6e9e8dec-fe7f-46c1-dd60-8c1122751764"
      },
      "source": [
        "classification_report = classification_report(y_test, predicted_classes)\n",
        "print(classification_report)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.82      0.82    159998\n",
            "           1       0.82      0.82      0.82    160002\n",
            "\n",
            "    accuracy                           0.82    320000\n",
            "   macro avg       0.82      0.82      0.82    320000\n",
            "weighted avg       0.82      0.82      0.82    320000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MiBHX7-qa_q"
      },
      "source": [
        "lstm_precision = 0.82\n",
        "lstm_recall = 0.82\n",
        "lstm_f1 = 0.82\n",
        "\n",
        "bonus_lstm2_metrics = [lstm_precision, lstm_recall, lstm_f1]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoBN0o5PVRxI"
      },
      "source": [
        "#### Remove Usermentions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbMYuXOEVRxI"
      },
      "source": [
        "tokenizer = TweetTokenizer(strip_handles=True)\n",
        "result = tokenizer.tokenize(str(traindata.tweet_text))\n",
        "traindata['no_usermention'] = traindata.tweet_text.apply(lambda x: ' '.join(tokenizer.tokenize(x)))\n",
        "X_train = traindata['no_usermention'].tolist()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, traindata.polarity.tolist(),test_size=0.10,random_state=2361)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwxGR3q3VRxI"
      },
      "source": [
        "X_train_encode=[]\n",
        "for tweet in X_train:\n",
        "    xtrain_list = []\n",
        "    for i in tweet.split(' '):\n",
        "      try:\n",
        "        xtrain_list.append(word_index[i])\n",
        "      except:\n",
        "          continue\n",
        "    X_train_encode.append(xtrain_list)\n",
        "X_test_encode=[]\n",
        "for ttweet in X_test:\n",
        "    xtest_list = []\n",
        "    for i in ttweet.split(' '):\n",
        "        try:\n",
        "          xtest_list.append(word_index[i])            \n",
        "        except:\n",
        "          continue\n",
        "    X_test_encode.append(xtest_list)\n",
        "#converting back the train_test split list variable to nDarray\n",
        "# X_train, X_test, y_train, y_test = np.ravel(X_train), np.ravel(X_test), np.ravel(y_train), np.ravel(y_test)\n",
        "X_train = np.array(X_train_encode, dtype=object)\n",
        "X_test = np.array(X_test_encode, dtype=object) \n",
        "y_train = np.array(y_train, dtype=object)\n",
        "y_test = np.array(y_test, dtype=object)\n",
        "\n",
        "# The length of tweets are of maximum 280 characters\n",
        "review_length = 280\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen = review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen = review_length)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRvpGC0uVRxI",
        "outputId": "dd73a828-305d-4f8b-aea5-eb832fc81d7b"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = len(word_index), # The size of our vocabulary \n",
        "        output_dim = 32, # Dimensions to which each words shall be mapped\n",
        "        input_length = review_length # Length of input sequences\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.LSTM(\n",
        "        units=32 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 \n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=1, # Single unit\n",
        "        activation='sigmoid' # Sigmoid activation function (output from 0 to 1)\n",
        "    )\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 280, 32)           43217536  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 280, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 43,225,889\n",
            "Trainable params: 43,225,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jml63_AwUFPF",
        "outputId": "18485200-4ea0-4262-cb4a-58551857bef1"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1440000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehyfGzqRUb3U",
        "outputId": "6357ef4b-de3d-4489-ce7f-f264500723f9"
      },
      "source": [
        "len(y_train)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1440000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJIsShBxVRxI"
      },
      "source": [
        "y_tensor = tf.convert_to_tensor(X_train, dtype=tf.int32)\n",
        "y_tensor1 = tf.convert_to_tensor(y_train, dtype=tf.int32)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zu7p4SrT9f1",
        "outputId": "51d9510b-0577-445b-84eb-2ee03267c7dd"
      },
      "source": [
        "history = model.fit(y_tensor, y_tensor1,batch_size=256,epochs=3,validation_split=0.2,verbose=1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "4500/4500 [==============================] - 1754s 387ms/step - loss: 0.4838 - accuracy: 0.7835 - val_loss: 0.4080 - val_accuracy: 0.8090\n",
            "Epoch 2/3\n",
            "4500/4500 [==============================] - 1752s 389ms/step - loss: 0.3456 - accuracy: 0.8285 - val_loss: 0.4130 - val_accuracy: 0.8027\n",
            "Epoch 3/3\n",
            "4500/4500 [==============================] - 1771s 391ms/step - loss: 0.2725 - accuracy: 0.8458 - val_loss: 0.4443 - val_accuracy: 0.8167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe2nhzjHVRxI",
        "outputId": "d9792f9e-12d1-48cd-edf6-5fd1eeb34b80"
      },
      "source": [
        "# predicted_classes = model.predict_classes(X_test)\n",
        "# classification_report = classification_report(y_test, predicted_classes)\n",
        "# print(classification_report)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "warnings.warn('`model.predict_classes()` is deprecated and '\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84     0.81     80144\n",
            "           1       0.83      0.80     0.80     79856\n",
            "\n",
            "    accuracy                          0.81     1440000\n",
            "   macro avg       0.82      0.82     0.81     1440000\n",
            "weighted avg       0.82      0.82     0.81     1440000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7R7qRNsqRO9"
      },
      "source": [
        "# considering weighted avg\n",
        "lstm_precision = 0.82\n",
        "lstm_recall = 0.82\n",
        "lstm_f1 = 0.81\n",
        "\n",
        "bonus_lstm3_metrics = [lstm_precision, lstm_recall, lstm_f1]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "CEmL-bFjrocE",
        "outputId": "bc0b7696-fd3f-4865-920a-0c2ae3f44dd3"
      },
      "source": [
        "df_bonus = pd.DataFrame({'bonus_lstm1_metrics':bonus_lstm1_metrics, 'bonus_lstm2_metrics':bonus_lstm2_metrics, 'bonus_lstm3_metrics':bonus_lstm3_metrics})\n",
        "df_bonus"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bonus_lstm1_metrics</th>\n",
              "      <th>bonus_lstm2_metrics</th>\n",
              "      <th>bonus_lstm3_metrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.81</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bonus_lstm1_metrics  bonus_lstm2_metrics  bonus_lstm3_metrics\n",
              "0                 0.82                 0.82                 0.82\n",
              "1                 0.82                 0.82                 0.82\n",
              "2                 0.82                 0.82                 0.81"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn4_8a7PqQAu"
      },
      "source": [
        "### Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVh-svaAVRxJ"
      },
      "source": [
        "#### Did this change the result in any way?\n",
        "Not drastically.\n",
        "\n",
        "\n",
        "#### Why do you think so?\n",
        "The usernames and stopwords are not major companents of sentiment analysis process. They do not contribute to sentiment analysis process."
      ]
    }
  ]
}