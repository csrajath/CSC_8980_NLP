{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csc_8980_NLP_exam1.ipynb",
      "provenance": [],
      "mount_file_id": "1Y-6VmPaaLYM-l-nsI8LIVJVKc34MYmLE",
      "authorship_tag": "ABX9TyNS++lJBXlWi19iQJUVLsol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csrajath/CSC_8980_NLP/blob/main/csc_8980_NLP_exam1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVG6efrmXIIk"
      },
      "source": [
        "Name: **Rajath Chikkatur Srinivasa** \\\n",
        "PantherID: **002552425**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17XSH3-mawPG"
      },
      "source": [
        "# importing required libraries\n",
        "import re, random, os, nltk,spacy, os\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from nltk import bigrams, trigrams\n",
        "# all sklearn imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import *\n",
        "# importing models\n",
        "#importing classifier types\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KahLmbpejtjX"
      },
      "source": [
        "# unzipping dataset\n",
        "# !unzip /content/exam1_dataset.zip\n",
        "# !tar -xvf /content/exam1_dataset.zip\n",
        "!unzip /content/drive/MyDrive/Projects/NLP_Exam/exam1_dataset.zip"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1pWQFr1XMUN"
      },
      "source": [
        "**Question 1) (20 points) Write a generic function that takes: Classification algorithm name, vectorization method name, training set with labels as parameters (total of 3 parameters should be passed). The function should take the classification algorithm name, the vectorization method’s name, and the training set and train the desired model. Use the default training parameters for the models we have seen in class. This function should return the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOsOBLLxavwf"
      },
      "source": [
        "def generic_fun(algo_name, vec_method, train_set):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvINHMcqctm2"
      },
      "source": [
        "**Question 2) (30 points) Using the function from question 1 to build the following models:\n",
        "a) Model a: Naive Bayes, Vectorizer: TFIDF and Bag of Words, Training set should be 75% of the provided dataset. Leaving the remaining 25% for testing.\n",
        "b) Model b: RandomForest, Vectorizer: TFIDF and Bag of Words, Training set should be 70% of the provided dataset. Leaving the remaining 30% for testing.\n",
        "c) Model c: Support Vector Machines (SVC in sklearn), Vectorizer: TFIDF and Bag of Words, Training set should be 60% of the provided dataset. Leaving the remaining 40% for testing. NOTE: Set the random seed to: 12345. This needs to be consistently set to train the model AND split the data in test and train. If this is not done correctly, you will lose points as your answers will not be comparable with the grading key**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWrft3_-c4R1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py3M1FvOdGlr"
      },
      "source": [
        "**Question 3) (30 points) Using the models from Question 2, evaluate each model with its respective training set (for model a, that set is 25% of the data, for model b, that set is 30% of the data, and for model c that set is 40% of the data. Be careful to not mix up the evaluation sets. With the predictions on the test set and show the following metrics: Accuracy, Precision, Recall, and Macro F1-score. With this in mind, please write and answer these questions in your notebook:\n",
        "a) What model performs the best and why? (which metrics do you base this on, and why do you think it performs better than others).\n",
        "b) Why is it important not to mix up the testing sets between different models? Think about this one.\n",
        "c) Display in a single sorted dataframe (model name, training %, test %, accuracy, precision, recall, F1-score) all performance metrics, sorted by accuracy in descending\n",
        "manner**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U77sB2WvdOkw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udnEkSnSdPlT"
      },
      "source": [
        "**Question 4) (15 points) Using the documents in the folder named UNLABELED, please use your best performing trained model from question 3 to predict their labels. Please do this individually for each document. Print to the screen the following items: Document Name, Predicted Label and using a text cell, write your own opinion if the label is correct and why - note you have to read the document to make your own opinion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAKwv4nVdX0r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P5o5EjOdYqy"
      },
      "source": [
        "**Question 5) (20 points) Build a function that takes the set of documents as input and returns a cosine similarity matrix for those documents. Feed all documents in the TRAINING folder to this matrix. Instead of printing the returned cosine similarity matrix, create a heatmap plot from the returned matrix. Make sure your plot is nicely scaled, properly labeled, and uses a nice color range to show the similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoIHFDPojNQX"
      },
      "source": [
        "# creating set of documents i.e. list of all documents present in TRAINING folder\n",
        "folder_path = '/content/exam1_dataset/TRAINING/'\n",
        "file_content_corpus = []\n",
        "# print(distance())\n",
        "sub_folders = []\n",
        "for i in os.walk(folder_path):\n",
        "  sub_folders.append(i[0])\n",
        "for folder in sub_folders:\n",
        "  files = os.walk(folder).__next__()[-1]   \n",
        "  for f in files:\n",
        "    if not f.startswith('.'):                                                      \n",
        "      file_content_corpus.append(open(os.path.join(folder, f),'r').read()) "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UgIKCBlddG_"
      },
      "source": [
        "#driver code / function\n",
        "def distance(docset):\n",
        "  vec = TfidfVectorizer()\n",
        "  X = vec.fit_transform(docset)\n",
        "  cos_s = cosine_similarity(X, X, dense_output=False)\n",
        "  return cos_s"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL4A_qH9ICvN"
      },
      "source": [
        "# function output stored in conf_mat variable\n",
        "cos_mat = distance(file_content_corpus)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1zbkOlPrQLI"
      },
      "source": [
        "cos_mat.todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFXt0_yRlF01"
      },
      "source": [
        "# #creating heatmap\n",
        "# plt.figure(figsize=(6,7))\n",
        "sns.heatmap(cos_mat, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD8riMJ1dhP3"
      },
      "source": [
        "**Question 6) (15 points) Write a function that takes a cosine similarity matrix as input and returns a list with the top n document paris and their similarity. Note that you should only keep the document pairs that are unique and remove the comparisons of the document to itself. Print the top 50 similar document pairs. Compare the assigned class for each document and answer: Do all similar documents belong to the same class? Why or why not?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhn4Y1Aidl1C"
      },
      "source": [
        "def top_n_similar_works(cos_matrix, n):\n",
        "  topNSimilar = []\n",
        "  np.fill_diagonal(cos_matrix, 0)\n",
        "  index_1d = cos_matrix.flatten().argsort()[-(2*n):]\n",
        "  x_idx, y_idx = np.unravel_index(index_1d, cos_matrix.shape)\n",
        "  for i in range(0,len(x_idx),2):\n",
        "    x = x_idx[i]\n",
        "    y = y_idx[i]\n",
        "    if(x != y):\n",
        "      topNSimilar.append((x,y,cos_matrix[x][y]))\n",
        "  return topNSimilar[::-1]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxnhYZgYnBU5"
      },
      "source": [
        "print('top 50 similar works\\n')\n",
        "for i in top_n_similar_works(cos_s,50):\n",
        "  print(filenames[i[0]], 'is similar to', filenames[i[1]], '', i[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS5-DNwEdmkt"
      },
      "source": [
        "**Question 7) (20 points) Using Spacy’s part of speech tagger, process all sentences (hint: don’t forget to split the reviews) and count how many NOUN and VERB tags are found in all the movies review (TRAINING folder) separating them by label. In other words, how many NOUN and VERB tags are found in positive reviews, and how many NOUN and VERB tags are found in negative reviews. Answer the following question: When comparing both, do you see any differences? Why do you think about the differences? Or lack of them**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ekZfYKds9H"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# total number of nouns and verbs in positive reviews\n",
        "positive_reviews_files_path = \"/content/exam1_dataset/TRAINING/positive/\"\n",
        "# v_count = []\n",
        "# n_count = []\n",
        "p_dict = {}\n",
        "v_count = 0\n",
        "n_count = 0\n",
        "# p_count = 0\n",
        "for i in os.listdir(positive_reviews_files_path):\n",
        "  file_content = open(os.path.join(positive_reviews_files_path,i), 'r').read()\n",
        "  doc = nlp(file_content)\n",
        "  for i in doc:\n",
        "    if i.pos_=='VERB':\n",
        "      v_count+=1\n",
        "    elif i.pos_=='NOUN':\n",
        "      n_count+=1\n",
        "    elif i.pos_ == 'PUNCT':\n",
        "      if i in p_dict:\n",
        "        p_dict[i.text]+=1\n",
        "      else:\n",
        "        p_dict[i.text]=1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B1tV01lErlh",
        "outputId": "60830d72-a615-466c-cd76-85843d5e168d"
      },
      "source": [
        "print('Total number of NOUNs present in positive reviews are {}'.format(v_count))\n",
        "print('Total number of VERBS present in positive reviews are {}'.format(n_count))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of NOUNs present in positive reviews are 342125\n",
            "Total number of VERBS present in positive reviews are 542978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu1kFwiyFcNF"
      },
      "source": [
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "# total number of nouns and verbs in negative reviews\n",
        "negative_reviews_files = \"/content/exam1_dataset/TRAINING/negative/\"\n",
        "# v_count1 = []\n",
        "# n_count1 = []\n",
        "p_count1 = {}\n",
        "v_count1 = 0\n",
        "n_count1 = 0\n",
        "count1 = 0\n",
        "for i in os.listdir(negative_reviews_files):\n",
        "  file_content = open(os.path.join(negative_reviews_files,i), 'r').read()\n",
        "  doc = nlp(file_content)\n",
        "  for i in doc:\n",
        "    if i.pos_=='VERB':\n",
        "      v_count1+=1\n",
        "    elif i.pos_=='NOUN':\n",
        "      n_count1+=1\n",
        "    elif i.pos_ == 'PUNCT':\n",
        "      if i in p_count1:\n",
        "        p_count1[i.text]+=1\n",
        "      else:\n",
        "        p_count1[i.text]=1"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWeaLBchIm12",
        "outputId": "62f67349-dfd3-4974-a3ca-94fc85202d98"
      },
      "source": [
        "print('Total number of NOUNs present in positive reviews are {}'.format(v_count1))\n",
        "print('Total number of VERBS present in positive reviews are {}'.format(n_count1))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of NOUNs present in positive reviews are 355559\n",
            "Total number of VERBS present in positive reviews are 528475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWJWIRNBxg8v"
      },
      "source": [
        "- When comparing both, do you see any differences?\n",
        "  - When I compare the count of NOUNs and VERBs  between POSITIVE and NEGATIVE folder - Yes there is a difference. When comparing them indivisually within each folder / dataset also there is a difference.\n",
        "- Why do you think about the differences? Or lack of them\n",
        "  - The difference is primarility because of the nature of the review. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVf0soncd2Jc"
      },
      "source": [
        "**Question 8) (20 points) Using the results from the PoS process in question 7, count how many different PUNCT tags are found and their respective counts from the full dataset provided (both negative and positives together). Using regex, write a set of regular expressions that generate the same counts from the dataset without using NLTK or Spacy, just regex. Can you get the same counts? If not, why do you think this is?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "1MHgqY2Ld71N",
        "outputId": "d972c84c-861f-4f8d-b2e1-7b9bb890299a"
      },
      "source": [
        "total_punct = len(p_count1) + len(p_dict)\n",
        "print('Total number of different PUNCT tags found is {}\\n'.format(total_punct))\n",
        "df = pd.DataFrame(p_dict.items(), columns=['PUNCT', 'Count'])\n",
        "df1 = pd.DataFrame(p_count1.items(), columns=['PUNCT', 'Count'])\n",
        "df_f = pd.concat([df, df1], ignore_index=True, sort=False)\n",
        "df8 = df_f.groupby('PUNCT')['Count'].value_counts().to_frame()\n",
        "df8.columns = ['Counts']\n",
        "df8 = df8.reset_index().drop('Count', 1).drop_duplicates()\n",
        "print('The respective counts from the full dataset for each punct is provided in the below dataframe\\n')\n",
        "df8"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of different PUNCT tags found is 8462\n",
            "\n",
            "The respective counts from the full dataset for each punct is provided in the below dataframe\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PUNCT</th>\n",
              "      <th>Counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>!</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>!!!&lt;br</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>!!!and</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>!.&lt;br</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>!&lt;br</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6967</th>\n",
              "      <td>’s</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6968</th>\n",
              "      <td>“</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6969</th>\n",
              "      <td>”</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6970</th>\n",
              "      <td>…</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6971</th>\n",
              "      <td>₤250,000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6972 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         PUNCT  Counts\n",
              "0            !       2\n",
              "1       !!!<br       1\n",
              "2       !!!and       1\n",
              "3        !.<br       1\n",
              "4         !<br       2\n",
              "...        ...     ...\n",
              "6967        ’s       2\n",
              "6968         “       2\n",
              "6969         ”       2\n",
              "6970         …       1\n",
              "6971  ₤250,000       1\n",
              "\n",
              "[6972 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77fexlQnt1Sa",
        "outputId": "b4780b72-17b8-4d0b-ac13-84d457bf8dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Solving PUNT using RE\n",
        "# merging the file contents to one string\n",
        "file_content_corpus_merge = [' '.join(file_content_corpus[:])]\n",
        "# the login used here for RE is that everything apart from word character and space is a PUNCT\n",
        "punct_re = r'[^\\w\\s]'\n",
        "reCount = re.findall(punct_re,file_content_corpus_merge[0])\n",
        "print('{}'.format(len(reCount)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1329527"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BOPyJagPlNl"
      },
      "source": [
        "- Can you get the same counts? If not, why do you think this is?\n",
        "  - No, it is not the same count when done with regular expression and Spacy. Because, Spacy's coverage is both rule-based and contextual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL1yyTRfd8hQ"
      },
      "source": [
        "**Bonus Question: (40 points) Using the code from Class 09 - Word Embeddings, pre-tune BERT in order to classify movie reviews. You can use the full TRAINING folder for the tuning and use the UNLABELED folder for your final classification/prediction task. Do the label predictions from BERT match what your classifier from Question 4 predicted? If they don’t, are they better? Please say why. NOTE: This is a pro-level task and while you might be tempted to copy code from the Internet, do not do it, as you will fail the class if you are plagiarizing. The code from the class can be tweaked to do the task with some adjustments. This is definitely not easy, but that is why it is worth an extra 40 points!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trDUQntkT366"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YCct3_LT8XO"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}