{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csc_8980_NLP_exam1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPSLoIN9PscmGnkXwzDa1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csrajath/CSC_8980_NLP/blob/main/csc_8980_NLP_exam1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVG6efrmXIIk"
      },
      "source": [
        "Name: **Rajath Chikkatur Srinivasa** \\\n",
        "PantherID: **002552425**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17XSH3-mawPG"
      },
      "source": [
        "# importing required libraries\n",
        "import re, random, os, nltk,spacy\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk import bigrams, trigrams\n",
        "# all sklearn imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import *\n",
        "# importing models\n",
        "#importing classifier types\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1pWQFr1XMUN"
      },
      "source": [
        "**Question 1) (20 points) Write a generic function that takes: Classification algorithm name, vectorization method name, training set with labels as parameters (total of 3 parameters should be passed). The function should take the classification algorithm name, the vectorization method’s name, and the training set and train the desired model. Use the default training parameters for the models we have seen in class. This function should return the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOsOBLLxavwf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvINHMcqctm2"
      },
      "source": [
        "**Question 2) (30 points) Using the function from question 1 to build the following models:\n",
        "a) Model a: Naive Bayes, Vectorizer: TFIDF and Bag of Words, Training set should be 75% of the provided dataset. Leaving the remaining 25% for testing.\n",
        "b) Model b: RandomForest, Vectorizer: TFIDF and Bag of Words, Training set should be 70% of the provided dataset. Leaving the remaining 30% for testing.\n",
        "c) Model c: Support Vector Machines (SVC in sklearn), Vectorizer: TFIDF and Bag of Words, Training set should be 60% of the provided dataset. Leaving the remaining 40% for testing. NOTE: Set the random seed to: 12345. This needs to be consistently set to train the model AND split the data in test and train. If this is not done correctly, you will lose points as your answers will not be comparable with the grading key**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWrft3_-c4R1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py3M1FvOdGlr"
      },
      "source": [
        "**Question 3) (30 points) Using the models from Question 2, evaluate each model with its respective training set (for model a, that set is 25% of the data, for model b, that set is 30% of the data, and for model c that set is 40% of the data. Be careful to not mix up the evaluation sets. With the predictions on the test set and show the following metrics: Accuracy, Precision, Recall, and Macro F1-score. With this in mind, please write and answer these questions in your notebook:\n",
        "a) What model performs the best and why? (which metrics do you base this on, and why do you think it performs better than others).\n",
        "b) Why is it important not to mix up the testing sets between different models? Think about this one.\n",
        "c) Display in a single sorted dataframe (model name, training %, test %, accuracy, precision, recall, F1-score) all performance metrics, sorted by accuracy in descending\n",
        "manner**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U77sB2WvdOkw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udnEkSnSdPlT"
      },
      "source": [
        "**Question 4) (15 points) Using the documents in the folder named UNLABELED, please use your best performing trained model from question 3 to predict their labels. Please do this individually for each document. Print to the screen the following items: Document Name, Predicted Label and using a text cell, write your own opinion if the label is correct and why - note you have to read the document to make your own opinion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAKwv4nVdX0r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P5o5EjOdYqy"
      },
      "source": [
        "**Question 5) (20 points) Build a function that takes the set of documents as input and returns a cosine similarity matrix for those documents. Feed all documents in the TRAINING folder to this matrix. Instead of printing the returned cosine similarity matrix, create a heatmap plot from the returned matrix. Make sure your plot is nicely scaled, properly labeled, and uses a nice color range to show the similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UgIKCBlddG_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD8riMJ1dhP3"
      },
      "source": [
        "**Question 6) (15 points) Write a function that takes a cosine similarity matrix as input and returns a list with the top n document paris and their similarity. Note that you should only keep the document pairs that are unique and remove the comparisons of the document to itself. Print the top 50 similar document pairs. Compare the assigned class for each document and answer: Do all similar documents belong to the same class? Why or why not?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhn4Y1Aidl1C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS5-DNwEdmkt"
      },
      "source": [
        "**Question 7) (20 points) Using Spacy’s part of speech tagger, process all sentences (hint: don’t forget to split the reviews) and count how many NOUN and VERB tags are found in all the movies review (TRAINING folder) separating them by label. In other words, how many NOUN and VERB tags are found in positive reviews, and how many NOUN and VERB tags are found in negative reviews. Answer the following question: When comparing both, do you see any differences? Why do you think about the differences? Or lack of them**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ekZfYKds9H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVf0soncd2Jc"
      },
      "source": [
        "**Question 8) (20 points) Using the results from the PoS process in question 7, count how many different PUNCT tags are found and their respective counts from the full dataset provided (both negative and positives together). Using regex, write a set of regular expressions that generate the same counts from the dataset without using NLTK or Spacy, just regex. Can you get the same counts? If not, why do you think this is?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MHgqY2Ld71N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL1yyTRfd8hQ"
      },
      "source": [
        "**Bonus Question: (40 points) Using the code from Class 09 - Word Embeddings, pre-tune BERT in order to classify movie reviews. You can use the full TRAINING folder for the tuning and use the UNLABELED folder for your final classification/prediction task. Do the label predictions from BERT match what your classifier from Question 4 predicted? If they don’t, are they better? Please say why. NOTE: This is a pro-level task and while you might be tempted to copy code from the Internet, do not do it, as you will fail the class if you are plagiarizing. The code from the class can be tweaked to do the task with some adjustments. This is definitely not easy, but that is why it is worth an extra 40 points!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvkuxcW1eDv8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}