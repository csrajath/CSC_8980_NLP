{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rcs1_TNLP_CSC_8980_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1a1MwRe-7gPTcOGw2MLnjCVjla1Kqlk2j",
      "authorship_tag": "ABX9TyM99QaBbSkkBey4dL2eoWB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csrajath/CSC_8980_NLP/blob/main/rcs1_TNLP_CSC_8980_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkhrxC02k2wi"
      },
      "source": [
        "### Rajath Chikkatur Srinivasa\n",
        "### 002552425"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YB4x8E4lB8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf26953-7ca4-4ca9-d4c3-411a2954e049"
      },
      "source": [
        "# importing libraries\n",
        "import nltk, os\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from IPython.display import clear_output\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "# models\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#sentiment analysis\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIS5Gk5AlD9f"
      },
      "source": [
        "# unzipping dataset\n",
        "!tar -xvf /content/drive/MyDrive/Projects/NLP/review_polarity.tar.gz\n",
        "clear_output()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rnX7oDPk67u"
      },
      "source": [
        "1. Using NLTK tokenize all documents, separated by polarity, remove stop words, and list the top 20 most frequent tokens (and their counts) for the positive reviews, and the top 20 most frequent tokens (and their counts). What kind of things do you notice are different between the two sets? (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojtbIf-hw9Zw",
        "outputId": "141bfb9b-2c5a-4cf5-8751-93267aefb627"
      },
      "source": [
        "# collecting stopwords and punkts\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nAtpahJ0ZH7"
      },
      "source": [
        "pos_filepath = '/content/txt_sentoken/pos/'\n",
        "neg_filepath = '/content/txt_sentoken/neg/'\n",
        "pos_content = []\n",
        "neg_content = []"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV6a_X_g66JN"
      },
      "source": [
        "for i in os.listdir(pos_filepath):\n",
        "    file_content = open(os.path.join(pos_filepath,i), 'r').read()\n",
        "    pos_content.append(file_content)\n",
        "complete_pos_content = \" \".join(pos_content)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXcy0oQr6qjr"
      },
      "source": [
        "for i in os.listdir(neg_filepath):\n",
        "    file_content = open(os.path.join(neg_filepath,i), 'r').read()\n",
        "    neg_content.append(file_content)\n",
        "complete_neg_content = \" \".join(neg_content)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBFfUhsT5gy5"
      },
      "source": [
        "#tokenizing positive\n",
        "pos_tokens = word_tokenize(complete_pos_content) \n",
        "#tokenizing negative\n",
        "neg_tokens = word_tokenize(complete_neg_content) "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GXSk6h181L2"
      },
      "source": [
        "#remove stopwords from positive tokens\n",
        "rsw_pos_tokens = [word for word in pos_tokens if word.lower() not in stop_words]\n",
        "#remove stopwords from negative tokens\n",
        "rsw_neg_tokens = [word for word in neg_tokens if word.lower() not in stop_words]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wIOaK6PD1Lk"
      },
      "source": [
        "p= defaultdict(int)\n",
        "for i in rsw_pos_tokens:\n",
        "    p[i]+=1\n",
        "n= defaultdict(int)\n",
        "for i in rsw_neg_tokens:\n",
        "    n[i]+=1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dM_sOBLEN_W"
      },
      "source": [
        "pos_sort = sorted(p.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "neg_sort = sorted(n.items(), key=lambda x: x[1], reverse=True)[:20]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QngIpR1U5hi0",
        "outputId": "5590c3bc-fae7-4a83-ab58-9180280877e6"
      },
      "source": [
        "print(\"top 20 most frequent tokens and their count for positive reviews are:\")\n",
        "for i in pos_sort:\n",
        "  print('token:', i[0], 'count:',i[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top 20 most frequent tokens and their count for positive reviews are:\n",
            "token: , count: 42448\n",
            "token: . count: 33714\n",
            "token: 's count: 9473\n",
            "token: `` count: 8494\n",
            "token: ) count: 6039\n",
            "token: ( count: 6014\n",
            "token: film count: 5186\n",
            "token: one count: 2943\n",
            "token: n't count: 2775\n",
            "token: movie count: 2497\n",
            "token: like count: 1713\n",
            "token: ? count: 1570\n",
            "token: : count: 1502\n",
            "token: story count: 1231\n",
            "token: also count: 1200\n",
            "token: good count: 1190\n",
            "token: even count: 1175\n",
            "token: time count: 1171\n",
            "token: would count: 1079\n",
            "token: character count: 1067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGDh-EA1GD0X",
        "outputId": "4c2bd748-948a-49e4-f3e1-58c4930a7faa"
      },
      "source": [
        "print(\"top 20 most frequent tokens and their count for negative reviews are:\")\n",
        "for j in neg_sort:\n",
        "  print('token:', j[0], 'count:',j[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top 20 most frequent tokens and their count for negative reviews are:\n",
            "token: , count: 35269\n",
            "token: . count: 32162\n",
            "token: `` count: 9123\n",
            "token: 's count: 8655\n",
            "token: ) count: 5742\n",
            "token: ( count: 5650\n",
            "token: film count: 4257\n",
            "token: n't count: 3442\n",
            "token: movie count: 3174\n",
            "token: one count: 2637\n",
            "token: ? count: 2201\n",
            "token: like count: 1832\n",
            "token: : count: 1540\n",
            "token: even count: 1381\n",
            "token: would count: 1185\n",
            "token: good count: 1126\n",
            "token: time count: 1111\n",
            "token: ! count: 1056\n",
            "token: get count: 1039\n",
            "token: bad count: 1019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru3B0iUkGW6I"
      },
      "source": [
        "* What kind of things do you notice are different between the two sets?\n",
        "- Firstly the sentiment terms count and the term itself is different. This is obvious. For example: good v/s bad\n",
        "- There are some positive tokens that are appearing the negative sentiment list like good.\n",
        "- The ordering of the terms are different. \n",
        "- There are some terms present in positive list but not in negative list and vice-versa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm9IFCKglxbv"
      },
      "source": [
        "2. Using the code from previous lectures, build 3 polarity classifiers using the following parameters (20 points). Note: just train the models.\\\n",
        "a) For training: use 50% of the positive dataset and 70% of the negative dataset. For your model use: NaiveBayes with the TF-IDF vectorizer.\\\n",
        "b) For training: use 50% of the negative dataset and 70% of the positive dataset. For your model use: NaiveBayes with the TF-IDF vectorizer.\\\n",
        "c) For training: use 25% of the negative dataset and 25% of the positive dataset. For your model use: SVM with the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOiK54oRJTFm"
      },
      "source": [
        "a) For training: use 50% of the positive dataset and 70% of the negative dataset. For your model use: NaiveBayes with the TF-IDF vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwl_qh_LlyOL"
      },
      "source": [
        "random.shuffle(pos_content)\n",
        "random.shuffle(neg_content)\n",
        "pos_m1 = len(pos_content[:int((50/100)*len(pos_content))])\n",
        "neg_m1 = len(neg_content[:int((70/100)*len(neg_content))])\n",
        "m1_pos_train = pos_content[:pos_m1]\n",
        "m1_neg_train = neg_content[:neg_m1]\n",
        "m1_pos_test = pos_content[pos_m1:]\n",
        "m1_neg_test = neg_content[neg_m1:]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PegRFQGGi9W",
        "outputId": "97e9afd3-d537-4b4d-c8fd-faa6f775ea46"
      },
      "source": [
        "m1_train_data = m1_pos_train + m1_neg_train\n",
        "m1_train_labels_1 = [1 for i in m1_pos_train]\n",
        "m1_train_labels_0 = [0 for i in m1_neg_train]\n",
        "m1_test_labels_1 = [1 for i in m1_pos_test]\n",
        "m1_test_labels_0 = [0 for i in m1_neg_test]\n",
        "m1_train_labels = m1_train_labels_1 + m1_train_labels_0 # 500-pos; 700-neg\n",
        "m1_test_labels = m1_test_labels_1 + m1_test_labels_0 # 500-pos; 300-neg\n",
        "#training NaiveBayes1\n",
        "model_nb_1 = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_nb_1.fit(m1_train_data, m1_train_labels)\n",
        "# labels = model.predict(test.data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhRTGvQjJIth"
      },
      "source": [
        "b) For training: use 50% of the negative dataset and 70% of the positive dataset. For your model use: NaiveBayes with the TF-IDF vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8ZwLgaQcWa5"
      },
      "source": [
        "random.shuffle(pos_content)\n",
        "random.shuffle(neg_content)\n",
        "pos_m2 = len(pos_content[:int((70/100)*len(pos_content))])\n",
        "neg_m2 = len(neg_content[:int((50/100)*len(neg_content))])\n",
        "m2_pos_train = pos_content[:pos_m2]\n",
        "m2_neg_train = neg_content[:neg_m2]\n",
        "m2_pos_test = pos_content[pos_m2:]\n",
        "m2_neg_test = neg_content[neg_m2:]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDYP3V30GkQ9",
        "outputId": "19a92cce-6728-4e06-fe40-7349ddb85bd6"
      },
      "source": [
        "#training NaiveBayes2\n",
        "m2_train_data = m2_pos_train + m2_neg_train\n",
        "m2_train_labels_1 = [1 for i in m2_pos_train]\n",
        "m2_train_labels_0 = [0 for i in m2_neg_train]\n",
        "m2_test_labels_1 = [1 for i in m2_pos_test]\n",
        "m2_test_labels_0 = [0 for i in m2_neg_test]\n",
        "m2_train_labels = m2_train_labels_1 + m2_train_labels_0\n",
        "# m2_test_labels = m2_test_labels_1 + m2_test_labels_0\n",
        "model_nb_2 = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_nb_2.fit(m2_train_data, m2_train_labels)\n",
        "# labels = model.predict(test.data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41cmoVOIJMPl"
      },
      "source": [
        "c) For training: use 25% of the negative dataset and 25% of the positive dataset. For your model use: SVM with the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q00hWVtTck5_"
      },
      "source": [
        "random.shuffle(pos_content)\n",
        "random.shuffle(neg_content)\n",
        "pos_m3 = len(pos_content[:int((25/100)*len(pos_content))])\n",
        "neg_m3 = len(neg_content[:int((25/100)*len(neg_content))])\n",
        "m3_pos_train = pos_content[:pos_m3]\n",
        "m3_neg_train = neg_content[:neg_m3]\n",
        "m3_pos_test = pos_content[pos_m3:]\n",
        "m3_neg_test = neg_content[neg_m3:]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWDbVINoIF7e",
        "outputId": "8f473d10-aa77-485e-e1b8-f10bef42c453"
      },
      "source": [
        "#training SVM\n",
        "m3_train_data = m3_pos_train + m3_neg_train\n",
        "m3_train_labels_1 = [1 for i in m3_pos_train]\n",
        "m3_train_labels_0 = [0 for i in m3_neg_train]\n",
        "m3_test_labels_1 = [1 for i in m3_pos_test]\n",
        "m3_test_labels_0 = [0 for i in m3_neg_test]\n",
        "m3_train_labels = m3_train_labels_1 + m3_train_labels_0\n",
        "m3_test_labels = m3_test_labels_1 + m3_test_labels_0\n",
        "model_svc_3 = make_pipeline(TfidfVectorizer(), SVC())\n",
        "model_svc_3.fit(m3_train_data, m3_train_labels)\n",
        "# labels = model.predict(test.data)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YgfGMhulyZa"
      },
      "source": [
        "3) Using the models from question 2, evaluate them on their individual rest of the dataset. This is, \\\n",
        "for a) 50% positive and 30% negative, \\\n",
        "for b) 50% negative and 30% positive, and \\\n",
        "for c) 75% negative and 75% positive. Calculate and show ONLY the following metrics for each model: Accuracy, Precision, Recall, Macro F1-score. (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6BEOQkTlysR"
      },
      "source": [
        "# printing performance metrics for model-1\n",
        "m1_name = 'NaiveBayes1-TfIdf'\n",
        "m1_pred = model_nb_1.predict(m1_pos_test+m1_neg_test)\n",
        "m1_accuracy = accuracy_score(m1_test_labels, m1_pred)\n",
        "m1_precision = precision_score(m1_test_labels, m1_pred)\n",
        "recall_model1 =  recall_score(m1_test_labels, m1_pred)\n",
        "mf1_model_1 =  f1_score(m1_pred, m1_test_labels, average='macro')\n",
        "m1_pvalues = [m1_name, m1_accuracy, m1_precision, recall_model1, mf1_model_1]\n",
        "# print('Mean Absolute Error:', mean_absolute_error(test.target,labels))\n",
        "# print('Mean Squared Error:', mean_squared_error(test.target, labels))  \n",
        "# print('Root Mean Squared Error:', np.sqrt(mean_squared_error(test.target, labels)))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmOk07qeJjiq"
      },
      "source": [
        "# printing performance metrics for model-2\n",
        "m2_test_labels = m2_test_labels_1 + m2_test_labels_0\n",
        "m2_name = 'NaiveBayes2-TfIdf'\n",
        "m2_pred = model_nb_2.predict(m2_pos_test+m2_neg_test)\n",
        "m2_accuracy = accuracy_score(m2_test_labels,m2_pred)\n",
        "m2_precision = precision_score(m2_test_labels, m2_pred,m2_test_labels)\n",
        "recall_model2 =  recall_score(m2_test_labels,m2_pred)\n",
        "mf1_model_2 =  f1_score(m2_pred, m2_test_labels, average='macro')\n",
        "m2_pvalues = [m2_name, m2_accuracy, m2_precision, recall_model2, mf1_model_2]\n",
        "# print('Mean Absolute Error:', mean_absolute_error(test.target,labels))\n",
        "# print('Mean Squared Error:', mean_squared_error(test.target, labels))  \n",
        "# print('Root Mean Squared Error:', np.sqrt(mean_squared_error(test.target, labels)))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGThnruoJqXn"
      },
      "source": [
        "# printing performance metrics for model-3\n",
        "m3_test_labels = m3_test_labels_1 + m3_test_labels_0\n",
        "m3_name = 'Support Vector Machine'\n",
        "m3_pred = model_svc_3.predict(m3_pos_test+m3_neg_test)\n",
        "m3_accuracy = accuracy_score(m3_test_labels, m3_pred)\n",
        "m3_precision = precision_score(m3_test_labels, m3_pred)\n",
        "recall_model3 =  recall_score(m3_test_labels, m3_pred)\n",
        "mf1_model_3 =  f1_score(m3_pred,m3_test_labels, average='macro')\n",
        "m3_pvalues = [m3_name, m3_accuracy, m3_precision, recall_model3, mf1_model_3]\n",
        "# print('Mean Absolute Error:', mean_absolute_error(test.target,labels))\n",
        "# print('Mean Squared Error:', mean_squared_error(test.target, labels))  \n",
        "# print('Root Mean Squared Error:', np.sqrt(mean_squared_error(test.target, labels)))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "bLTFrNxXTKZP",
        "outputId": "5495be01-1af9-4188-fa7c-3fc07e5346b3"
      },
      "source": [
        "#creating a dataframe for metrics of all models performance metrixs\n",
        "pm_list = [m3_pvalues, m2_pvalues, m1_pvalues]\n",
        "pm_df = pd.DataFrame(pm_list, columns = [\"model_name\", \"accuracy\", \"precison\", \"recall\", 'macro_f1_score'])\n",
        "pm_df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precison</th>\n",
              "      <th>recall</th>\n",
              "      <th>macro_f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>0.78400</td>\n",
              "      <td>0.768262</td>\n",
              "      <td>0.813333</td>\n",
              "      <td>0.783814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaiveBayes2-TfIdf</td>\n",
              "      <td>0.37625</td>\n",
              "      <td>0.375469</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.274971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaiveBayes1-TfIdf</td>\n",
              "      <td>0.37625</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.274971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               model_name  accuracy  precison    recall  macro_f1_score\n",
              "0  Support Vector Machine   0.78400  0.768262  0.813333        0.783814\n",
              "1       NaiveBayes2-TfIdf   0.37625  0.375469  1.000000        0.274971\n",
              "2       NaiveBayes1-TfIdf   0.37625  1.000000  0.002000        0.274971"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVm25QQVly3q"
      },
      "source": [
        "4) Using the model performance metrics from question 3, answer the following questions. Please provide logical and intuitive rationale for your answers, simple answers like: because it has the best score, will not be sufficient. (40 points):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwW18z4nz1Y4"
      },
      "source": [
        "a) What is the best performing model?\\\n",
        "Support vector machines is the best performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehb01-O7z4dU"
      },
      "source": [
        "b) Why do you think this is the best performing model?\\\n",
        "The training set for the category of sentiments (positve and negative) in SVM is equally distributed that is well balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q5-iKTH0AWF"
      },
      "source": [
        "c) How does class imbalance play in determining polarity?\\\n",
        "Since model1 and model2 had class imbalances. It can be seen from the derived metrics for them that precision and recall respectively for each of them is calculated as 1 which is not an ideal scinario for a classification task. This is also reflected in their accuracy scores which is low. Over all, with respect to polarity - Model 1 and Model 2 , due to class imbalance, classifies their own polarity(higher percentage polarity dataset) correctly.  In this case, Negative(Precision-1 in model1) and Positive (Precision-1 in model2) respectively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpt_yMKc0Ccy"
      },
      "source": [
        "d) Do you think either more data or a better model is a better approach for this\n",
        "kind of task?\\\n",
        "More data does not always guarentee better performance/output. It is a blend of having the correct model with right amount of data. Sometimes a better performing model provides incorrect output due to improper balance in dataset. An hybrid model approach can be incorporated. Transformer models, Neural network based models can be trained to achieve "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud20FJI_lzXv"
      },
      "source": [
        "5) Using NLTK and VADER, calculate the sentiment score for all documents in the\n",
        "positive polarity. Calculate the polarity threshold needed (and reasonable) to have the majority of the document labels match. Do the same for the negative class. Provide the threshold needed, the reason why you think this threshold is reasonable, and the accuracy percentage (how many documents are correctly labeled using this threshold). (45 points):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3aX9zXDPhW-"
      },
      "source": [
        "sia = SIA()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZtje6IUOMex"
      },
      "source": [
        "def get_scores(content, filename):\n",
        "    sia_scores = sia.polarity_scores(content)\n",
        "    \n",
        "    return pd.Series({\n",
        "        'filename': filename,\n",
        "        'compound': sia_scores['compound'],\n",
        "        'positive': sia_scores['pos'],\n",
        "        'neutral': sia_scores['neu'],\n",
        "        'negative': sia_scores['neg']\n",
        "    })"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKOxqYQ2lzsn"
      },
      "source": [
        "df_pos_scores = pd.DataFrame([])\n",
        "\n",
        "for i in os.listdir(pos_filepath):\n",
        "    file_content = open(os.path.join(pos_filepath,i), 'r').read()\n",
        "    df_1 = get_scores(file_content, i).to_frame().transpose()\n",
        "    for index, row in df_1.iterrows():\n",
        "      df_pos_scores = df_pos_scores.append({'file_name': row['filename'], 'compound': row['compound'],'positive': row['positive'], 'neutral': row['neutral'], 'negative': row['negative']}, ignore_index = True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "n1j4X_2SpzHX",
        "outputId": "f0e71629-3b05-448f-f93d-e04b0da27c01"
      },
      "source": [
        "print('The sentiment score for all documents in the positive polarity is saved in the below dataframe')\n",
        "df_pos_scores"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sentiment score for all documents in the positive polarity is saved in the below dataframe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>compound</th>\n",
              "      <th>file_name</th>\n",
              "      <th>negative</th>\n",
              "      <th>neutral</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.9956</td>\n",
              "      <td>cv684_11798.txt</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.9955</td>\n",
              "      <td>cv990_11591.txt</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.9957</td>\n",
              "      <td>cv187_12829.txt</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.8974</td>\n",
              "      <td>cv441_13711.txt</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.9997</td>\n",
              "      <td>cv967_5788.txt</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.734</td>\n",
              "      <td>0.207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.9997</td>\n",
              "      <td>cv521_15828.txt</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.671</td>\n",
              "      <td>0.249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.9959</td>\n",
              "      <td>cv822_20049.txt</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.9965</td>\n",
              "      <td>cv710_22577.txt</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.817</td>\n",
              "      <td>0.138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>-0.9331</td>\n",
              "      <td>cv771_28665.txt</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.771</td>\n",
              "      <td>0.112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>-0.9569</td>\n",
              "      <td>cv162_10424.txt</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.095</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     compound        file_name  negative  neutral  positive\n",
              "0      0.9956  cv684_11798.txt     0.078    0.761     0.162\n",
              "1      0.9955  cv990_11591.txt     0.064    0.827     0.109\n",
              "2      0.9957  cv187_12829.txt     0.062    0.764     0.174\n",
              "3      0.8974  cv441_13711.txt     0.119    0.749     0.132\n",
              "4      0.9997   cv967_5788.txt     0.059    0.734     0.207\n",
              "..        ...              ...       ...      ...       ...\n",
              "995    0.9997  cv521_15828.txt     0.080    0.671     0.249\n",
              "996    0.9959  cv822_20049.txt     0.059    0.801     0.139\n",
              "997    0.9965  cv710_22577.txt     0.045    0.817     0.138\n",
              "998   -0.9331  cv771_28665.txt     0.117    0.771     0.112\n",
              "999   -0.9569  cv162_10424.txt     0.103    0.802     0.095\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCI9v-9zD2me"
      },
      "source": [
        "df_neg_scores = pd.DataFrame([])\n",
        "\n",
        "for i in os.listdir(neg_filepath):\n",
        "    file_content = open(os.path.join(neg_filepath,i), 'r').read()\n",
        "    df_2 = get_scores(file_content, i).to_frame().transpose()\n",
        "    for index, row in df_2.iterrows():\n",
        "      df_neg_scores = df_neg_scores.append({'file_name': row['filename'], 'compound': row['compound'],'positive': row['positive'], 'neutral': row['neutral'], 'negative': row['negative']}, ignore_index = True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "uEMJeyBaD2xM",
        "outputId": "e6694adb-b043-4a7a-d6b3-cb419a5b643b"
      },
      "source": [
        "print('The sentiment score for all documents in the negative polarity is saved in the below dataframe')\n",
        "df_neg_scores"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sentiment score for all documents in the negative polarity is saved in the below dataframe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>compound</th>\n",
              "      <th>file_name</th>\n",
              "      <th>negative</th>\n",
              "      <th>neutral</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.5308</td>\n",
              "      <td>cv496_11185.txt</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.9941</td>\n",
              "      <td>cv265_11625.txt</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.730</td>\n",
              "      <td>0.116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.7614</td>\n",
              "      <td>cv436_20564.txt</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.820</td>\n",
              "      <td>0.084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.9300</td>\n",
              "      <td>cv708_28539.txt</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.9926</td>\n",
              "      <td>cv130_18521.txt</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.726</td>\n",
              "      <td>0.089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>-0.9781</td>\n",
              "      <td>cv570_28960.txt</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.822</td>\n",
              "      <td>0.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.9446</td>\n",
              "      <td>cv110_27832.txt</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.717</td>\n",
              "      <td>0.168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.8970</td>\n",
              "      <td>cv983_24219.txt</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.9780</td>\n",
              "      <td>cv493_14135.txt</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.838</td>\n",
              "      <td>0.111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>-0.9864</td>\n",
              "      <td>cv615_15734.txt</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.076</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     compound        file_name  negative  neutral  positive\n",
              "0     -0.5308  cv496_11185.txt     0.102    0.806     0.092\n",
              "1     -0.9941  cv265_11625.txt     0.153    0.730     0.116\n",
              "2     -0.7614  cv436_20564.txt     0.096    0.820     0.084\n",
              "3     -0.9300  cv708_28539.txt     0.119    0.794     0.087\n",
              "4     -0.9926  cv130_18521.txt     0.184    0.726     0.089\n",
              "..        ...              ...       ...      ...       ...\n",
              "995   -0.9781  cv570_28960.txt     0.113    0.822     0.065\n",
              "996    0.9446  cv110_27832.txt     0.115    0.717     0.168\n",
              "997    0.8970  cv983_24219.txt     0.105    0.760     0.135\n",
              "998    0.9780  cv493_14135.txt     0.051    0.838     0.111\n",
              "999   -0.9864  cv615_15734.txt     0.118    0.806     0.076\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyW8HDn2lwaR"
      },
      "source": [
        "Provide the threshold needed, the reason why you think this threshold is reasonable, and the accuracy percentage (how many documents are correctly labeled using this threshold). (45 points):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TpP42XflvbF"
      },
      "source": [
        "# one way of calculating the threshold is using the mean of values for each polarity and derive standard deviation to deduce accuracy\n",
        "df_pos_mean = df_pos_scores['positive'].mean()\n",
        "df_pos_mean = df_pos_scores['negative'].mean()\n",
        "df_neg_mean = df_neg_scores['negative'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ-oq5yBl6tz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vdYOtZWmh8O"
      },
      "source": [
        "Bonus (40 points): Repeat questions 2,3 and 4 removing all stopwords. Answer the\n",
        "following questions: Did this change the results in any way? Why do you think so?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xm3e_bUmlTh"
      },
      "source": [
        "pos_content_bonus = []\n",
        "# removing stop words for positive review\n",
        "# rsw_pos_tokens = [word for word in pos_tokens if word.lower() not in stop_words]\n",
        "for i in pos_content:\n",
        "  sw_tokens_pos = []\n",
        "  doc_tokens = nltk.word_tokenize(i)\n",
        "  for token in doc_tokens:\n",
        "    if token not in stop_words:\n",
        "      sw_tokens_pos.append(token)\n",
        "  pos_content_bonus.append(' '.join(sw_tokens_pos))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCBBBsv4FrI1"
      },
      "source": [
        "neg_content_bonus = []\n",
        "# removing stop words for positive review\n",
        "# rsw_pos_tokens = [word for word in pos_tokens if word.lower() not in stop_words]\n",
        "for i in neg_content:\n",
        "  sw_tokens_neg = []\n",
        "  doc_tokens = nltk.word_tokenize(i)\n",
        "  for token in doc_tokens:\n",
        "    if token not in stop_words:\n",
        "      sw_tokens_neg.append(token)\n",
        "  neg_content_bonus.append(' '.join(sw_tokens_neg))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZA5A-dZ8BCY"
      },
      "source": [
        "a) For training: use 50% of the positive dataset and 70% of the negative dataset. For your model use: NaiveBayes with the TF-IDF vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSxuYnXetHi9",
        "outputId": "c96b9440-9fa2-443d-c998-886964491172"
      },
      "source": [
        "random.shuffle(pos_content_bonus)\n",
        "random.shuffle(neg_content_bonus)\n",
        "\n",
        "pos_m3 = len(pos_content_bonus[:int((50/100)*len(pos_content_bonus))])\n",
        "neg_m3 = len(neg_content_bonus[:int((70/100)*len(neg_content_bonus))])\n",
        "m1_pos_train = pos_content_bonus[:pos_m3]\n",
        "m1_neg_train = neg_content_bonus[:neg_m3]\n",
        "m1_pos_test = pos_content_bonus[pos_m3:]\n",
        "m1_neg_test = neg_content_bonus[neg_m3:]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "700"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgSuxajktP1Z",
        "outputId": "a11cf0e6-77e9-42f7-9638-8e24157604c7"
      },
      "source": [
        "m1_train_data = m1_pos_train + m1_neg_train\n",
        "m1_train_labels_1 = [1 for i in m1_pos_train]\n",
        "m1_train_labels_0 = [0 for i in m1_neg_train]\n",
        "m1_test_labels_1 = [1 for i in m1_pos_test]\n",
        "m1_test_labels_0 = [0 for i in m1_neg_test]\n",
        "m1_train_labels = m1_train_labels_1 + m1_train_labels_0 # 500-pos; 700-neg\n",
        "m1_test_labels = m1_test_labels_1 + m1_test_labels_0 # 500-pos; 300-neg\n",
        "#training NaiveBayes1 for bonus\n",
        "model_nb_1 = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_nb_1.fit(m1_train_data, m1_train_labels)\n",
        "# labels = model.predict(test.data)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0CmOCAfO8fi"
      },
      "source": [
        "Evaluate above models on their individual rest of the dataset. This is,\n",
        "for 50% positive and 30% negative,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEOmyQi2O2BY"
      },
      "source": [
        "# printing performance metrics for model-1\n",
        "m1_name = 'NaiveBayes1-TfIdf'\n",
        "m1_pred = model_nb_1.predict(m1_pos_test+m1_neg_test)\n",
        "m1_accuracy = accuracy_score(m1_test_labels, m1_pred)\n",
        "m1_precision = precision_score(m1_test_labels, m1_pred)\n",
        "recall_model1 =  recall_score(m1_test_labels, m1_pred)\n",
        "mf1_model_1 =  f1_score(m1_pred, m1_test_labels, average='macro')\n",
        "m1_pvalues = [m1_name, m1_accuracy, m1_precision, recall_model1, mf1_model_1]"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtHJ_0oeJWo6"
      },
      "source": [
        "b) For training: use 50% of the negative dataset and 70% of the positive dataset. For your model use: NaiveBayes with the TF-IDF vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PU5BJKBJRuK"
      },
      "source": [
        "random.shuffle(pos_content_bonus)\n",
        "random.shuffle(neg_content_bonus)\n",
        "\n",
        "pos_m3 = len(pos_content_bonus[:int((70/100)*len(pos_content_bonus))])\n",
        "neg_m3 = len(neg_content_bonus[:int((50/100)*len(neg_content_bonus))])\n",
        "m2_pos_train = pos_content_bonus[:pos_m3]\n",
        "m2_neg_train = neg_content_bonus[:neg_m3]\n",
        "m2_pos_test = pos_content_bonus[pos_m3:]\n",
        "m2_neg_test = neg_content_bonus[neg_m3:]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwfuwqlLJU7T",
        "outputId": "2cbf49ec-a5ad-4288-eae0-d351ecc385ea"
      },
      "source": [
        "#training NaiveBayes2 for bonus\n",
        "m2_train_data = m2_pos_train + m2_neg_train\n",
        "m2_train_labels_1 = [1 for i in m2_pos_train]\n",
        "m2_train_labels_0 = [0 for i in m2_neg_train]\n",
        "m2_test_labels_1 = [1 for i in m2_pos_test]\n",
        "m2_test_labels_0 = [0 for i in m2_neg_test]\n",
        "m2_train_labels = m2_train_labels_1 + m2_train_labels_0\n",
        "# m2_test_labels = m2_test_labels_1 + m2_test_labels_0\n",
        "model_nb_2 = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_nb_2.fit(m2_train_data, m2_train_labels)\n",
        "# labels = model.predict(test.data)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('multinomialnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ugYlWYjPHM1"
      },
      "source": [
        "Evaluate above models on their individual rest of the dataset. This is,\n",
        "for b) 50% negative and 30% positive,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrTHjMwFO0_G"
      },
      "source": [
        "# printing performance metrics for model-2\n",
        "m2_test_labels = m2_test_labels_1 + m2_test_labels_0\n",
        "m2_name = 'NaiveBayes2-TfIdf'\n",
        "m2_pred = model_nb_2.predict(m2_pos_test+m2_neg_test)\n",
        "m2_accuracy = accuracy_score(m2_test_labels,m2_pred)\n",
        "m2_precision = precision_score(m2_test_labels, m2_pred,m2_test_labels)\n",
        "recall_model2 =  recall_score(m2_test_labels,m2_pred)\n",
        "mf1_model_2 =  f1_score(m2_pred, m2_test_labels, average='macro')\n",
        "m2_pvalues = [m2_name, m2_accuracy, m2_precision, recall_model2, mf1_model_2]"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vCuC3-IMTdQ"
      },
      "source": [
        "c) For training: use 25% of the negative dataset and 25% of the positive dataset. For your model use: SVM with the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub_kUgSFMXLZ"
      },
      "source": [
        "random.shuffle(pos_content_bonus)\n",
        "random.shuffle(neg_content_bonus)\n",
        "\n",
        "pos_m3 = len(pos_content_bonus[:int((25/100)*len(pos_content_bonus))])\n",
        "neg_m3 = len(neg_content_bonus[:int((25/100)*len(neg_content_bonus))])\n",
        "m3_pos_train = pos_content_bonus[:pos_m3]\n",
        "m3_neg_train = neg_content_bonus[:neg_m3]\n",
        "m3_pos_test = pos_content_bonus[pos_m3:]\n",
        "m3_neg_test = neg_content_bonus[neg_m3:]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbsN-oJ2MXYw",
        "outputId": "2116c6e8-dc4d-4ff7-b7f1-e7469f9288a2"
      },
      "source": [
        "#training SVM fo rbonus\n",
        "m3_train_data = m3_pos_train + m3_neg_train\n",
        "m3_train_labels_1 = [1 for i in m3_pos_train]\n",
        "m3_train_labels_0 = [0 for i in m3_neg_train]\n",
        "m3_test_labels_1 = [1 for i in m3_pos_test]\n",
        "m3_test_labels_0 = [0 for i in m3_neg_test]\n",
        "m3_train_labels = m3_train_labels_1 + m3_train_labels_0\n",
        "m3_test_labels = m3_test_labels_1 + m3_test_labels_0\n",
        "model_svc_3 = make_pipeline(TfidfVectorizer(), SVC())\n",
        "model_svc_3.fit(m3_train_data, m3_train_labels)\n",
        "# labels = model.predict(test.data)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nMMcpYgPf_s"
      },
      "source": [
        "Evaluate above models on their individual rest of the dataset. This is, for b) 50% negative and 30% positive,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFy0sEdAPif7"
      },
      "source": [
        "# printing performance metrics for model-3\n",
        "m3_test_labels = m3_test_labels_1 + m3_test_labels_0\n",
        "m3_name = 'Support Vector Machine'\n",
        "m3_pred = model_svc_3.predict(m3_pos_test+m3_neg_test)\n",
        "m3_accuracy = accuracy_score(m3_test_labels, m3_pred)\n",
        "m3_precision = precision_score(m3_test_labels, m3_pred)\n",
        "recall_model3 =  recall_score(m3_test_labels, m3_pred)\n",
        "mf1_model_3 =  f1_score(m3_pred,m3_test_labels, average='macro')\n",
        "m3_pvalues = [m3_name, m3_accuracy, m3_precision, recall_model3, mf1_model_3]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "W3YN8wmoOpMN",
        "outputId": "fff5dc2b-46ab-4061-e5d0-083c7a215e1e"
      },
      "source": [
        "#creating a dataframe for metrics of all models performance metrixs\n",
        "pm_list_bonus = [m3_pvalues, m2_pvalues, m1_pvalues]\n",
        "pm_df_bonus = pd.DataFrame(pm_list_bonus, columns = [\"model_name\", \"accuracy\", \"precison\", \"recall\", 'macro_f1_score'])\n",
        "pm_df_bonus"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precison</th>\n",
              "      <th>recall</th>\n",
              "      <th>macro_f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>0.791333</td>\n",
              "      <td>0.793289</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.791331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaiveBayes2-TfIdf</td>\n",
              "      <td>0.376250</td>\n",
              "      <td>0.375469</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.274971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaiveBayes1-TfIdf</td>\n",
              "      <td>0.376250</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.274971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               model_name  accuracy  precison  recall  macro_f1_score\n",
              "0  Support Vector Machine  0.791333  0.793289   0.788        0.791331\n",
              "1       NaiveBayes2-TfIdf  0.376250  0.375469   1.000        0.274971\n",
              "2       NaiveBayes1-TfIdf  0.376250  1.000000   0.002        0.274971"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLUwH1NJq-TP"
      },
      "source": [
        "a) What is the best performing model?\n",
        "Support vector machines is the best performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGwthHX2rC4r"
      },
      "source": [
        "b) Why do you think this is the best performing model?\\\n",
        "Because their classes are well balances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2q_E2A6r4fX"
      },
      "source": [
        "c) How does class imbalance play in determining polarity?\\\n",
        "Class imbalance leads to low accuracy which is seen in model 1(naivebayes1) and model 2 (naivebayes2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM5a_lADr6oP"
      },
      "source": [
        "d) Do you think either more data or a better model is a better approach for this kind of task?\n",
        "As mentioned in Q4, increasing the data does not guarentee better results. In fact it might overfit/underfit. With respect to models, one can try advanced models like Neural Networks based models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzC-yuNQqktD"
      },
      "source": [
        "e) Did this change the results in any way? Why do you think so?\\\n",
        "No so, because stop words does not majorly determine the polarity of a term. That is usually not how the sentiment libraries determine polarity.  The same stop words would be present in either/both dataset (positive and negative)"
      ]
    }
  ]
}