{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rcs1_TNLP_CSC_8980_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1a1MwRe-7gPTcOGw2MLnjCVjla1Kqlk2j",
      "authorship_tag": "ABX9TyNCdSdh5focohBKJ4+K4CaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csrajath/CSC_8980_NLP/blob/main/rcs1_TNLP_CSC_8980_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkhrxC02k2wi"
      },
      "source": [
        "### Rajath Chikkatur Srinivasa\n",
        "### 002552425"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YB4x8E4lB8L"
      },
      "source": [
        "# importing libraries\n",
        "import nltk, os\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from IPython.display import clear_output\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# models\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#sentiment analysis\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIS5Gk5AlD9f"
      },
      "source": [
        "# unzipping dataset\n",
        "!tar -xvf /content/drive/MyDrive/Projects/NLP/review_polarity.tar.gz\n",
        "clear_output()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rnX7oDPk67u"
      },
      "source": [
        "1. Using NLTK tokenize all documents, separated by polarity, remove stop words, and list the top 20 most frequent tokens (and their counts) for the positive reviews, and the top 20 most frequent tokens (and their counts). What kind of things do you notice are different between the two sets? (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojtbIf-hw9Zw",
        "outputId": "25eb1936-1549-4cb0-e38a-fdcd4ad1dd6f"
      },
      "source": [
        "# collecting stopwords and punkts\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nAtpahJ0ZH7"
      },
      "source": [
        "pos_filepath = '/content/txt_sentoken/pos/'\n",
        "neg_filepath = '/content/txt_sentoken/neg/'\n",
        "pos_content = []\n",
        "neg_content = []"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV6a_X_g66JN"
      },
      "source": [
        "for i in os.listdir(pos_filepath):\n",
        "    file_content = open(os.path.join(pos_filepath,i), 'r').read()\n",
        "    pos_content.append(file_content)\n",
        "complete_pos_content = \" \".join(pos_content)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXcy0oQr6qjr"
      },
      "source": [
        "for i in os.listdir(neg_filepath):\n",
        "    file_content = open(os.path.join(neg_filepath,i), 'r').read()\n",
        "    neg_content.append(file_content)\n",
        "complete_neg_content = \" \".join(neg_content)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBFfUhsT5gy5"
      },
      "source": [
        "#tokenizing positive\n",
        "pos_tokens = word_tokenize(complete_pos_content) \n",
        "#tokenizing negative\n",
        "neg_tokens = word_tokenize(complete_neg_content) "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GXSk6h181L2"
      },
      "source": [
        "#remove stopwords from positive tokens\n",
        "rsw_pos_tokens = [word for word in pos_tokens if word.lower() not in stop_words]\n",
        "#remove stopwords from negative tokens\n",
        "rsw_neg_tokens = [word for word in neg_tokens if word.lower() not in stop_words]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wIOaK6PD1Lk"
      },
      "source": [
        "p= defaultdict(int)\n",
        "for i in rsw_pos_tokens:\n",
        "    p[i]+=1\n",
        "n= defaultdict(int)\n",
        "for i in rsw_neg_tokens:\n",
        "    n[i]+=1"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dM_sOBLEN_W"
      },
      "source": [
        "pos_sort = sorted(p.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "neg_sort = sorted(n.items(), key=lambda x: x[1], reverse=True)[:20]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QngIpR1U5hi0",
        "outputId": "ace8c6ff-5501-49b6-d45e-ef5f93cd9d03"
      },
      "source": [
        "print(\"top 20 most frequent tokens and their count for positive reviews are:\")\n",
        "for i in pos_sort:\n",
        "  print('token:', i[0], 'count:',i[1])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top 20 most frequent tokens and their count for positive reviews are:\n",
            "token: , count: 42448\n",
            "token: . count: 33714\n",
            "token: 's count: 9473\n",
            "token: `` count: 8494\n",
            "token: ) count: 6039\n",
            "token: ( count: 6014\n",
            "token: film count: 5186\n",
            "token: one count: 2943\n",
            "token: n't count: 2775\n",
            "token: movie count: 2497\n",
            "token: like count: 1713\n",
            "token: ? count: 1570\n",
            "token: : count: 1502\n",
            "token: story count: 1231\n",
            "token: also count: 1200\n",
            "token: good count: 1190\n",
            "token: even count: 1175\n",
            "token: time count: 1171\n",
            "token: would count: 1079\n",
            "token: character count: 1067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGDh-EA1GD0X",
        "outputId": "414ecaf5-dd55-459c-9ec8-42817627058e"
      },
      "source": [
        "print(\"top 20 most frequent tokens and their count for negative reviews are:\")\n",
        "for j in neg_sort:\n",
        "  print('token:', j[0], 'count:',j[1])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top 20 most frequent tokens and their count for negative reviews are:\n",
            "token: , count: 35269\n",
            "token: . count: 32162\n",
            "token: `` count: 9123\n",
            "token: 's count: 8655\n",
            "token: ) count: 5742\n",
            "token: ( count: 5650\n",
            "token: film count: 4257\n",
            "token: n't count: 3442\n",
            "token: movie count: 3174\n",
            "token: one count: 2637\n",
            "token: ? count: 2201\n",
            "token: like count: 1832\n",
            "token: : count: 1540\n",
            "token: even count: 1381\n",
            "token: would count: 1185\n",
            "token: good count: 1126\n",
            "token: time count: 1111\n",
            "token: ! count: 1056\n",
            "token: get count: 1039\n",
            "token: bad count: 1019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru3B0iUkGW6I"
      },
      "source": [
        "* What kind of things do you notice are different between the two sets?\n",
        "- Firstly the sentiment terms count and the term itself is different. This is obvious. For example: good v/s bad\n",
        "- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm9IFCKglxbv"
      },
      "source": [
        "2. Using the code from previous lectures, build 3 polarity classifiers using the following parameters (20 points). Note: just train the models.\\\n",
        "a) For training: use 50% of the positive dataset and 70% of the negative dataset. For your model use: NaiveBayes with the TF-IDF vectorizer.\\\n",
        "b) For training: use 50% of the negative dataset and 70% of the positive dataset. For your model use: NaiveBayes with the TF-IDF vectorizer.\\\n",
        "c) For training: use 25% of the negative dataset and 25% of the positive dataset. For your model use: SVM with the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwl_qh_LlyOL"
      },
      "source": [
        "# polarity classifier models\n",
        "#pcm1\n",
        "\n",
        "pcm1 = True\n",
        "#pcm2\n",
        "\n",
        "pcm2 = True\n",
        "\n",
        "#pcm3\n",
        "\n",
        "pcm3 = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PegRFQGGi9W"
      },
      "source": [
        "#applying NaiveBayes\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model.fit(train.data, train.target)\n",
        "labels = model.predict(test.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDYP3V30GkQ9"
      },
      "source": [
        "#applying NaiveBayes\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model.fit(train.data, train.target)\n",
        "labels = model.predict(test.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWDbVINoIF7e"
      },
      "source": [
        "#applying SVM\n",
        "model = make_pipeline(TfidfVectorizer(), SVC())\n",
        "model.fit(train.data, train.target)\n",
        "labels = model.predict(test.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyeuyCPBGkY3"
      },
      "source": [
        "# printing performance metrics\n",
        "print('Accuracy:',accuracy_score(test.target,labels))\n",
        "print('Precision:', precision_score(test.target,labels))\n",
        "print('Recall:', recall_score(test.target,labels))\n",
        "print('F1 Score:', f1_score(labels, test.target, average='macro'))\n",
        "print('Mean Absolute Error:', mean_absolute_error(test.target,labels))\n",
        "print('Mean Squared Error:', mean_squared_error(test.target, labels))  \n",
        "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(test.target, labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YgfGMhulyZa"
      },
      "source": [
        "Using the models from question 2, evaluate them on their individual rest of the dataset. This is, \\\n",
        "for a) 50% positive and 30% negative, \\\n",
        "for b) 50% negative and 30% positive, and \\\n",
        "for c) 75% negative and 75% positive. Calculate and show ONLY the following metrics for each model: Accuracy, Precision, Recall, Macro F1-score. (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6BEOQkTlysR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVm25QQVly3q"
      },
      "source": [
        "4) Using the model performance metrics from question 3, answer the following questions. Please provide logical and intuitive rationale for your answers, simple answers like: because it has the best score, will not be sufficient. (40 points):\\\n",
        "a) What is the best performing model?\\\n",
        "b) Why do you think this is the best performing model?\\\n",
        "c) How does class imbalance play in determining polarity?\\\n",
        "d) Do you think either more data or a better model is a better approach for this\n",
        "kind of task?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrKQTZeUlzMr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud20FJI_lzXv"
      },
      "source": [
        "5) Using NLTK and VADER, calculate the sentiment score for all documents in the\n",
        "positive polarity. Calculate the polarity threshold needed (and reasonable) to have the majority of the document labels match. Do the same for the negative class. Provide the threshold needed, the reason why you think this threshold is reasonable, and the accuracy percentage (how many documents are correctly labeled using this threshold). (45 points):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3aX9zXDPhW-"
      },
      "source": [
        "sia = SIA()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZtje6IUOMex"
      },
      "source": [
        "def get_scores(content, filename):\n",
        "    sia_scores = sia.polarity_scores(content)\n",
        "    \n",
        "    return pd.Series({\n",
        "        'filename': filename,\n",
        "        'compound': sia_scores['compound'],\n",
        "        'positive': sia_scores['pos'],\n",
        "        'neutral': sia_scores['neu'],\n",
        "        'negative': sia_scores['neg']\n",
        "    })"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKOxqYQ2lzsn"
      },
      "source": [
        "df_pos_scores = pd.DataFrame([])\n",
        "\n",
        "for i in os.listdir(pos_filepath):\n",
        "    file_content = open(os.path.join(pos_filepath,i), 'r').read()\n",
        "    df_1 = get_scores(file_content, i).to_frame().transpose()\n",
        "    for index, row in df_1.iterrows():\n",
        "      df_pos_scores = df_pos_scores.append({'file_name': row['filename'], 'compound': row['compound'],'positive': row['positive'], 'neutral': row['neutral'], 'negative': row['negative']}, ignore_index = True)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "n1j4X_2SpzHX",
        "outputId": "f7cbc238-2684-4423-da00-2ced74e01c43"
      },
      "source": [
        "print('The sentiment score for all documents in the positive polarity is saved in the below dataframe')\n",
        "df_pos_scores"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sentiment score for all documents in the positive polarity is saved in the below dataframe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>compound</th>\n",
              "      <th>file_name</th>\n",
              "      <th>negative</th>\n",
              "      <th>neutral</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.9931</td>\n",
              "      <td>cv815_22456.txt</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.736</td>\n",
              "      <td>0.160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.9996</td>\n",
              "      <td>cv274_25253.txt</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0.204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.9210</td>\n",
              "      <td>cv927_10681.txt</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.852</td>\n",
              "      <td>0.079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.9993</td>\n",
              "      <td>cv442_13846.txt</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.9973</td>\n",
              "      <td>cv551_10565.txt</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.753</td>\n",
              "      <td>0.160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>-0.9568</td>\n",
              "      <td>cv117_24295.txt</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.674</td>\n",
              "      <td>0.157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>-0.9861</td>\n",
              "      <td>cv602_8300.txt</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.848</td>\n",
              "      <td>0.052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.9960</td>\n",
              "      <td>cv560_17175.txt</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>-0.9181</td>\n",
              "      <td>cv891_6385.txt</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.771</td>\n",
              "      <td>0.109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.9880</td>\n",
              "      <td>cv677_17715.txt</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     compound        file_name  negative  neutral  positive\n",
              "0      0.9931  cv815_22456.txt     0.104    0.736     0.160\n",
              "1      0.9996  cv274_25253.txt     0.083    0.713     0.204\n",
              "2      0.9210  cv927_10681.txt     0.069    0.852     0.079\n",
              "3     -0.9993  cv442_13846.txt     0.199    0.684     0.117\n",
              "4      0.9973  cv551_10565.txt     0.087    0.753     0.160\n",
              "..        ...              ...       ...      ...       ...\n",
              "995   -0.9568  cv117_24295.txt     0.169    0.674     0.157\n",
              "996   -0.9861   cv602_8300.txt     0.100    0.848     0.052\n",
              "997    0.9960  cv560_17175.txt     0.107    0.759     0.134\n",
              "998   -0.9181   cv891_6385.txt     0.120    0.771     0.109\n",
              "999    0.9880  cv677_17715.txt     0.101    0.772     0.127\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCI9v-9zD2me"
      },
      "source": [
        "df_neg_scores = pd.DataFrame([])\n",
        "\n",
        "for i in os.listdir(neg_filepath):\n",
        "    file_content = open(os.path.join(neg_filepath,i), 'r').read()\n",
        "    df_2 = get_scores(file_content, i).to_frame().transpose()\n",
        "    for index, row in df_2.iterrows():\n",
        "      df_neg_scores = df_neg_scores.append({'file_name': row['filename'], 'compound': row['compound'],'positive': row['positive'], 'neutral': row['neutral'], 'negative': row['negative']}, ignore_index = True)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "uEMJeyBaD2xM",
        "outputId": "804de792-fad3-4d21-ca0f-b00360be13e3"
      },
      "source": [
        "print('The sentiment score for all documents in the negative polarity is saved in the below dataframe')\n",
        "df_neg_scores"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sentiment score for all documents in the negative polarity is saved in the below dataframe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>compound</th>\n",
              "      <th>file_name</th>\n",
              "      <th>negative</th>\n",
              "      <th>neutral</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.8243</td>\n",
              "      <td>cv561_9484.txt</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.798</td>\n",
              "      <td>0.093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.9932</td>\n",
              "      <td>cv135_12506.txt</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.755</td>\n",
              "      <td>0.161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.5106</td>\n",
              "      <td>cv568_17065.txt</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.6371</td>\n",
              "      <td>cv297_10104.txt</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.732</td>\n",
              "      <td>0.135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.9387</td>\n",
              "      <td>cv523_18285.txt</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.833</td>\n",
              "      <td>0.100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>-0.9766</td>\n",
              "      <td>cv323_29633.txt</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.729</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.9981</td>\n",
              "      <td>cv574_23191.txt</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.8535</td>\n",
              "      <td>cv385_29621.txt</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.758</td>\n",
              "      <td>0.134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.9793</td>\n",
              "      <td>cv936_17473.txt</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.731</td>\n",
              "      <td>0.156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.9223</td>\n",
              "      <td>cv396_19127.txt</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.660</td>\n",
              "      <td>0.197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     compound        file_name  negative  neutral  positive\n",
              "0     -0.8243   cv561_9484.txt     0.109    0.798     0.093\n",
              "1      0.9932  cv135_12506.txt     0.083    0.755     0.161\n",
              "2     -0.5106  cv568_17065.txt     0.116    0.772     0.112\n",
              "3     -0.6371  cv297_10104.txt     0.134    0.732     0.135\n",
              "4      0.9387  cv523_18285.txt     0.067    0.833     0.100\n",
              "..        ...              ...       ...      ...       ...\n",
              "995   -0.9766  cv323_29633.txt     0.146    0.729     0.125\n",
              "996    0.9981  cv574_23191.txt     0.096    0.735     0.169\n",
              "997    0.8535  cv385_29621.txt     0.108    0.758     0.134\n",
              "998    0.9793  cv936_17473.txt     0.113    0.731     0.156\n",
              "999    0.9223  cv396_19127.txt     0.144    0.660     0.197\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vdYOtZWmh8O"
      },
      "source": [
        "Bonus (40 points): Repeat questions 2,3 and 4 removing all stopwords. Answer the\n",
        "following questions: Did this change the results in any way? Why do you think so?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xm3e_bUmlTh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn1wqtEXkr1_"
      },
      "source": [
        "# for x,y in zip(filepath, file_content):\n",
        "#   file_content = sia(y)\n",
        "#   df = df.append()\n"
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}